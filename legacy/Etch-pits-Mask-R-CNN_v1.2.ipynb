{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4784eac2-cdb7-4bb7-b550-c6cb819c6375",
   "metadata": {},
   "source": [
    "# Part 1: Imports\n",
    "- Import the necessary libraries for training and analysis, and define the paths to the images/masks folders to be used for training.\n",
    "- The naming convention is \"image_i\" and \"image_i_mask_j\".\n",
    "- Images used for training are in the .png format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a43040-c710-49ef-ac77-c3641a47ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# Uncomment last line to install PyTorch with CUDA (check compatible version) \n",
    "# If there are conflicting dependencies when installing with Anaconda, it can be installed directly from the notebook\n",
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507de43e-3546-4148-9817-48d676bfff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#Imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split,Subset\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision # Pre-trained Mask R-CNN model\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import to_pil_image, to_tensor\n",
    "from torchvision.ops import nms\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import tifffile\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, jaccard_score, accuracy_score, roc_auc_score,log_loss\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c70566-09f5-4970-9ea6-3af2624b3bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeding all the modules ensures that the random processes required for training can be reproduced deterministically.\n",
    "# This is needed for reporting, but not strictly necessary for predicting.\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # for multi-GPU\n",
    "\n",
    "    # Force deterministic algorithms\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1936a2-4fc3-4c61-baca-41c8921a1cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check that GPU is available for calculations, and set the \"device\" variable\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n",
    "# Check if GPU is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device: \",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0669020-ed3d-4c18-b546-18874cbb4fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to image and masks for training\n",
    "\n",
    "image_dir = r\"C:\\Users\\USER\\PROJECT\\TRAINING\\Images\"\n",
    "mask_dir = r\"C:\\Users\\USER\\PROJECT\\TRAINING\\Masks\"\n",
    "output_dir = r\"C:\\Users\\USER\\PROJECT\\Pre-process\"\n",
    "models_dir = r\"C:\\Users\\USER\\PROJECT\\Models\"\n",
    "test_image_dir = r\"C:\\Users\\USER\\PROJECT\\TRAINING\\ANALYSIS\\Test\\Images\"\n",
    "test_mask_dir = r\"C:\\Users\\USER\\PROJECT\\TRAINING\\ANALYSIS\\Test\\Masks\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b33916b-5811-4ffb-85b8-f157a46b81f5",
   "metadata": {},
   "source": [
    "# Part 2: Data pre-processing\n",
    "- Define the pre-processing of the data that will be used for the training of the model.\n",
    "- The pre-processing consists of:\n",
    "    - load the data into the correct shape for training. Some steps require the channel dimension or a dummy dimension to work; \n",
    "    - normalizing image values to the 0-1 range;\n",
    "    - splitting the image in overlapping squares (512x512 size) using a grid (5x3), maintaning the link with the corresponding masks;\n",
    "    - removing training data that is not fully in a crop; \n",
    "    - applying augmentation functions to improve generalization of the model.\n",
    "- The data is saved as a .pt file to free up memory (large training datasets can consume a lot of memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894025fb-4970-47bb-9d3e-b6fdacb2dbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(image_dir, mask_dir, output_dir, crop_size=(512, 512), grid_size=(2, 3), augment=False):\n",
    "    \"\"\"\n",
    "    Preprocess images and masks by cropping and saving them using a grid.\n",
    "\n",
    "    Args:\n",
    "        image_dir (list of str): List of paths to input images.\n",
    "        mask_dir (list of str): List of directories containing masks for each image.\n",
    "        output_dir (str): Directory to save preprocessed data.\n",
    "        crop_size (tuple): Size of the crops (H, W).\n",
    "        grid_size (tuple): Grid dimensions for cropping (rows, cols).\n",
    "        augment (bool): Presence/absence of augmentation function\n",
    "    \"\"\"\n",
    "    dataset = []  # List to hold all samples\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    crop_H, crop_W = crop_size\n",
    "\n",
    "    # List all images in the image directory\n",
    "    image_paths = sorted(glob.glob(os.path.join(image_dir, \"image_*\")))  # Match image_i format\n",
    "    \n",
    "    for image_path in image_paths:\n",
    "        # Extract image index\n",
    "        image_idx = os.path.basename(image_path).split(\"_\")[1].split(\".\")[0]  # Extract 'i' from 'image_i'\n",
    "        \n",
    "        # Match corresponding masks for the image\n",
    "        mask_pattern = os.path.join(mask_dir, f\"image_{image_idx}_mask_*\")\n",
    "        mask_files = sorted(glob.glob(mask_pattern))  # Match image_i_mask_j format\n",
    "\n",
    "        # Load the image\n",
    "        image = Image.open(image_path).convert(\"I\")\n",
    "        image_tensor = transforms.ToTensor()(image)  # Convert to tensor [C, H, W]\n",
    "\n",
    "        # Load masks\n",
    "        masks = []\n",
    "        for mask_path in mask_files:\n",
    "            mask = Image.open(mask_path).convert('L')\n",
    "            masks.append(np.array(mask, dtype=np.uint8))\n",
    "        masks = np.stack(masks, axis=0)  # Shape: [N, H, W]\n",
    "        masks_tensor = torch.tensor(masks, dtype=torch.uint8)  # [N, H, W]\n",
    "\n",
    "        # Generate dummy boxes for all masks\n",
    "        boxes = []\n",
    "        for mask in masks:\n",
    "            pos = np.where(mask > 0)\n",
    "            if pos[0].size > 0:  # Check for non-empty mask\n",
    "                xmin, ymin, xmax, ymax = np.min(pos[1]), np.min(pos[0]), np.max(pos[1]), np.max(pos[0])\n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "        boxes_tensor = torch.tensor(boxes, dtype=torch.float32)  # [N, 4]\n",
    "\n",
    "        # Assign dummy labels (1 for all instances, can be adjusted later)\n",
    "        labels_tensor = torch.ones((len(boxes),), dtype=torch.int64)  # [N]\n",
    "\n",
    "        # Split into crops\n",
    "        crops = split_image_and_targets(\n",
    "            image=image_tensor,\n",
    "            masks=masks_tensor,\n",
    "            boxes=boxes_tensor,\n",
    "            labels=labels_tensor,\n",
    "            grid_size=grid_size,\n",
    "            crop_size=crop_size,\n",
    "            augment=augment,\n",
    "            image_index=image_idx\n",
    "        )\n",
    "        # Append all crops to the dataset\n",
    "        dataset.extend(crops)\n",
    "        \n",
    "    # Save as .pt file\n",
    "    crop_filename = \"dataset_crops.pt\"\n",
    "    torch.save(dataset, os.path.join(output_dir, crop_filename))    \n",
    "\n",
    "    print(f\"Processed images -> Saved crops to {crop_filename}\")\n",
    "  \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea8719b-1eb5-4813-a54e-8f44459b4e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define augmentations\n",
    "augmentation_pipeline = T.Compose([\n",
    "    T.RandomHorizontalFlip(p=0.3),\n",
    "    T.RandomVerticalFlip(p=0.3),\n",
    "    T.RandomRotation(degrees=30),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    T.RandomResizedCrop(size=(512, 512), scale=(0.2, 1)),\n",
    "    T.GaussianBlur(kernel_size=(1, 25), sigma=(10., 15.)),\n",
    "    T.RandomPerspective(distortion_scale=0.2, p=0.35)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bdfbfc-3b73-4394-8fc5-53acc8eaea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_augmentation(crop_data):\n",
    "    \"\"\"\n",
    "    Apply augmentations to a crop (image, masks, boxes).\n",
    "\n",
    "    Args:\n",
    "        crop_data (dict): Contains 'image', 'masks', 'boxes', 'labels'.\n",
    "\n",
    "    Returns:\n",
    "        dict: Augmented crop data.\n",
    "    \"\"\"\n",
    "    image = to_pil_image(crop_data['image'])  # Convert tensor to PIL\n",
    "    augmented_image = augmentation_pipeline(image)  # Apply augmentations\n",
    "    crop_data['image'] = to_tensor(augmented_image)  # Convert back to tensor\n",
    "    return crop_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f197cc-4efe-48ac-b850-58ac050cccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image_and_targets(image, masks, boxes, labels, grid_size=(2, 3), crop_size=(512, 512), augment=False, image_index=0):\n",
    "    \"\"\"\n",
    "    Splits the image, masks, and bounding boxes into overlapping crops with optional augmentations.\n",
    "\n",
    "    Args:\n",
    "        image (Tensor): Original image tensor of shape [C, H, W].\n",
    "        masks (Tensor): Masks tensor of shape [N, H, W].\n",
    "        boxes (Tensor): Bounding boxes tensor of shape [N, 4].\n",
    "        labels (Tensor): Labels tensor of shape [N].\n",
    "        grid_size (tuple): (rows, cols) for splitting the image.\n",
    "        crop_size (tuple): (height, width) of each crop.\n",
    "        augment (bool): Whether to apply augmentations.\n",
    "        image_index (int): Index of the image in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing:\n",
    "              - 'image': Cropped image tensor [C, crop_H, crop_W].\n",
    "              - 'masks': Cropped masks tensor [N', crop_H, crop_W].\n",
    "              - 'boxes': Adjusted bounding boxes tensor [N', 4].\n",
    "              - 'labels': Cropped labels tensor [N'].\n",
    "              - 'metadata': Metadata dictionary containing:\n",
    "                            - 'image_index': Index of the original image.\n",
    "                            - 'x_start': x-coordinate of the crop's top-left corner.\n",
    "                            - 'y_start': y-coordinate of the crop's top-left corner.\n",
    "    \"\"\"\n",
    "    H, W = image.shape[1], image.shape[2]\n",
    "    crop_H, crop_W = crop_size\n",
    "    rows, cols = grid_size\n",
    "\n",
    "    stride_H = (H - crop_H) // (rows - 1) if rows > 1 else 0\n",
    "    stride_W = (W - crop_W) // (cols - 1) if cols > 1 else 0\n",
    "\n",
    "    crops = []\n",
    "\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            # Compute crop coordinates\n",
    "            y_start = min(row * stride_H, H - crop_H)\n",
    "            x_start = min(col * stride_W, W - crop_W)\n",
    "            y_end = y_start + crop_H\n",
    "            x_end = x_start + crop_W\n",
    "\n",
    "            # Crop the image\n",
    "            cropped_image = image[:, y_start:y_end, x_start:x_end]\n",
    "\n",
    "            # Adjust bounding boxes\n",
    "            cropped_masks = []\n",
    "            cropped_boxes = []\n",
    "            cropped_labels = []\n",
    "\n",
    "            for mask_idx, box in enumerate(boxes):\n",
    "                x_min, y_min, x_max, y_max = box\n",
    "                # Check if the box overlaps with the crop\n",
    "                if x_min < x_end and x_max > x_start and y_min < y_end and y_max > y_start:\n",
    "                    # Adjust box coordinates to crop-relative\n",
    "                    new_x_min = max(x_min - x_start, 0)\n",
    "                    new_y_min = max(y_min - y_start, 0)\n",
    "                    new_x_max = min(x_max - x_start, crop_W)\n",
    "                    new_y_max = min(y_max - y_start, crop_H)\n",
    "                    cropped_boxes.append([new_x_min, new_y_min, new_x_max, new_y_max])\n",
    "                    cropped_labels.append(labels[mask_idx].item())  # Convert label to int\n",
    "                    \n",
    "                    # Crop the corresponding mask\n",
    "                    cropped_mask = masks[mask_idx, y_start:y_end, x_start:x_end]\n",
    "                    cropped_mask = (cropped_mask > 0).float()\n",
    "                    cropped_masks.append(cropped_mask)\n",
    "\n",
    "            # Stack masks into a tensor if there are valid masks\n",
    "            if cropped_masks:\n",
    "                cropped_masks = torch.stack(cropped_masks, dim=0)\n",
    "            else:\n",
    "                cropped_masks = torch.zeros((0, crop_H, crop_W), dtype=torch.uint8)\n",
    "                \n",
    "            # Add metadata to the crop for reconstruction\n",
    "            metadata = {\n",
    "                \"image_index\": image_index,\n",
    "                \"x_start\": x_start,\n",
    "                \"y_start\": y_start,\n",
    "            }\n",
    "        \n",
    "            # Add to results if there are valid boxes\n",
    "            crop_data = {\n",
    "                    \"image\": cropped_image,\n",
    "                    \"masks\": cropped_masks,\n",
    "                    \"boxes\": torch.tensor(cropped_boxes, dtype=torch.float32),\n",
    "                    \"labels\": torch.tensor(cropped_labels, dtype=torch.int64),\n",
    "                    \"metadata\": metadata\n",
    "                }\n",
    "                \n",
    "            if augment:\n",
    "                crop_data = apply_augmentation(crop_data)\n",
    "\n",
    "            crops.append(crop_data)\n",
    "\n",
    "    return crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a75cf0-92e2-4792-a94b-e4e926c1b328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image(image, percentile_min=2, percentile_max=98, padding_value=0):\n",
    "    \"\"\"\n",
    "    Normalize the image while handling outliers and ignoring padded edges.\n",
    "    \n",
    "    Args:\n",
    "        image (torch.Tensor): Input image tensor.\n",
    "        percentile_min (float): Lower percentile for clipping (default: 2%).\n",
    "        percentile_max (float): Upper percentile for clipping (default: 98%).\n",
    "        padding_value (float): Value of padded background (default: 0).\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Normalized image tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Mask out padded regions\n",
    "    pad_mask = image != padding_value\n",
    "    if not pad_mask.any():\n",
    "        print(\"Warning: Entire image is padding. Returning zeros.\")\n",
    "        return torch.zeros_like(image)\n",
    "        \n",
    "    # Ensure the image is a float tensor\n",
    "    image = image.float()\n",
    "\n",
    "    # Extract valid pixel values (ignoring padding)\n",
    "    valid_pixels = image[pad_mask]\n",
    "    \n",
    "    # Compute percentile-based clipping bounds\n",
    "    min_val = torch.quantile(valid_pixels, percentile_min / 100.0)\n",
    "    max_val = torch.quantile(valid_pixels, percentile_max / 100.0)\n",
    "    \n",
    "    # Clip values to the specified percentiles\n",
    "    clipped_image = torch.clip(image, min=min_val, max=max_val)\n",
    "    \n",
    "    # Normalize to [0, 1], ignoring padding\n",
    "    normalized_image = (clipped_image - min_val) / (max_val - min_val)\n",
    "    normalized_image[~pad_mask] = 0  # Restore padding values to 0    \n",
    "    \n",
    "    # Avoid division by zero if all values are the same\n",
    "    if max_val == min_val:\n",
    "        print(\"Warning: Image has uniform intensity. Returning zeros.\")\n",
    "        return torch.zeros_like(image)\n",
    "\n",
    "    return normalized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37219aae-0776-4266-bd71-95a700f87e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can handle both data in memory or saved as a .pt file\n",
    "class ROIDataset(Dataset):\n",
    "    def __init__(self, dataset=None, data_path=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset (list or None): Preloaded dataset (list of dictionaries with 'image', 'masks', etc.).\n",
    "            data_path (str or None): Directory containing preprocessed `.pt` files.\n",
    "            transform (callable, optional): Optional transform to be applied to the image.\n",
    "        \"\"\"\n",
    "        if data_path:\n",
    "            self.data_path = data_path\n",
    "            self.data = torch.load(self.data_path)  # Data will be loaded from file\n",
    "        elif dataset:\n",
    "            self.data = dataset\n",
    "            self.data_path = None # No files to load\n",
    "        else:\n",
    "            raise ValueError(\"Either 'dataset' or 'data_path' must be provided.\")\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    @staticmethod\n",
    "    def is_mask_touching_border(mask, threshold=0, threshold_size=0):\n",
    "        \"\"\"\n",
    "        Check if a mask touches the border of an image for more than a threshold of pixels.\n",
    "        \"\"\"\n",
    "        \n",
    "        h, w = mask.shape\n",
    "        top_border = mask[0, :].sum()\n",
    "        bottom_border = mask[-1, :].sum()\n",
    "        left_border = mask[:, 0].sum()\n",
    "        right_border = mask[:, -1].sum()\n",
    "        mask_size = h*w\n",
    "        return (top_border > threshold or \n",
    "                bottom_border > threshold or \n",
    "                left_border > threshold or \n",
    "                right_border > threshold or\n",
    "                mask_size < threshold_size)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if self.data_path:\n",
    "            # Load preprocessed data from file \n",
    "            data = self.data[idx]\n",
    "        elif self.data:\n",
    "            # Use in-memory dataset\n",
    "            data = self.data[idx]\n",
    "        else:\n",
    "            raise RuntimeError(\"Dataset is not properly initialized.\")\n",
    "\n",
    "        image = data[\"image\"]\n",
    "        if self.transform:\n",
    "            if isinstance(image, torch.Tensor):\n",
    "                image = transforms.ToPILImage()(image)  # Convert back to PIL for transforms\n",
    "            image = self.transform(image)\n",
    "        image = normalize_image(image)\n",
    "        \n",
    "        # Filter masks that touch the border\n",
    "        masks = data[\"masks\"]\n",
    "        boxes = data[\"boxes\"]\n",
    "        labels = data[\"labels\"]\n",
    "            \n",
    "        valid_indices = []\n",
    "        for i, mask in enumerate(masks):\n",
    "            if not self.is_mask_touching_border(mask.numpy(), threshold=5, threshold_size=30):\n",
    "                valid_indices.append(i)\n",
    "                \n",
    "        # Apply filtering\n",
    "        masks = masks[valid_indices]\n",
    "        boxes = boxes[valid_indices]\n",
    "        labels = labels[valid_indices]\n",
    "        \n",
    "        # Skip if no valid masks remain\n",
    "        if len(valid_indices) == 0:\n",
    "            return None  # Indicate this crop should be skipped\n",
    "        \n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"masks\": masks,\n",
    "        }\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caf3f5f-ee43-4c82-a167-1565f0e08c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After having defined all the functions, the lines below uses them\n",
    "# This only needs to be run and saved once. If data was not previously pre-processed and saved, uncomment the line below\n",
    "\n",
    "#data = preprocess_data(image_dir, mask_dir, output_dir, crop_size=(512, 512), grid_size=(3, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fe0356-b828-45a7-a57e-b6738fa8ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "# We use Compose in case we want to add pre-processing steps later on\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create uncropped dataset and dataloader\n",
    "data_path = os.path.join(output_dir,\"dataset_crops.pt\")\n",
    "dataset = ROIDataset(data_path=data_path, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0f258f-1b42-4ec4-994f-8ff063c2fe85",
   "metadata": {},
   "source": [
    "# Part 3: Build dataloaders\n",
    "- With the input data in the correct shape, we can now build the training dataset in a way that the model can use it for training.\n",
    "- The data is split in training and validation datasets. This is repeated 5 times for k-fold cross-validation.\n",
    "- Crops from the same image need to be collectively assigned to the same dataset to prevent data leakage. \n",
    "- A few examples are visualized to check that the correct data is being loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99283d3-ca6a-42c1-abe1-c4972ce9164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    # Remove None entries\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    \n",
    "    # If batch is empty, return empty lists (skip this batch during training)\n",
    "    if len(batch) == 0:\n",
    "        print(\"Warning: empty batch encountered\")\n",
    "        return [], []\n",
    "    \n",
    "    images, targets = zip(*batch)\n",
    "    return list(images), list(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1956cc1-7d68-4766-a676-6a1cb49ba9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold cross-validation is needed to assess the model's metrics, but it requires running the training k times and on a reduced dataset.\n",
    "# It's possible to then choose the fold with the best metrics, or alternatively set k = 1 and re-run the training\n",
    "# In the second case, validation can be avoided since with the current setup it does not influence training. \n",
    "\n",
    "n_splits = 5       # Number of folds. Can be set to 1 for production training\n",
    "\n",
    "# Extract groups (image indices) from dataset metadata\n",
    "groups = [sample[\"metadata\"][\"image_index\"] for sample in dataset.data]\n",
    "\n",
    "if n_splits == 1:\n",
    "    # Production mode: use all data for training, no validation\n",
    "    splits = [(np.arange(len(dataset)), [])]  # train_idx = all, val_idx = empty\n",
    "else:\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    splits = list(gkf.split(X=np.zeros(len(groups)), groups=groups))\n",
    "\n",
    "# Loop over folds (works for both modes)\n",
    "for fold, (train_idx, val_idx) in enumerate(splits):\n",
    "    \n",
    "    # Build Subsets for train and validation\n",
    "    train_dataset = Subset(dataset, train_idx)\n",
    "    val_dataset   = Subset(dataset, val_idx) if len(val_idx) > 0 else None\n",
    "    \n",
    "    # Dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=8,\n",
    "        shuffle=True,\n",
    "        collate_fn=custom_collate_fn,\n",
    "        drop_last=True)\n",
    "    \n",
    "    val_loader = None\n",
    "    if val_dataset is not None:\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=8,\n",
    "            shuffle=False,\n",
    "            collate_fn=custom_collate_fn,\n",
    "            drop_last=True)\n",
    "    \n",
    "    # Print summary\n",
    "    train_groups = set(dataset.data[i][\"metadata\"][\"image_index\"] for i in train_idx)\n",
    "    val_groups   = set(dataset.data[i][\"metadata\"][\"image_index\"] for i in val_idx) if val_dataset else set()\n",
    "    overlap      = train_groups & val_groups\n",
    "    print(f\"Fold {fold+1}/{n_splits}: Train size={len(train_idx)}, \"\n",
    "          f\"Val size={len(val_idx)}, Overlap={overlap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd055632-3f0b-40ac-844b-cacf219ed733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization for both uncropped and cropped data\n",
    "def visualize_data(loader, num_samples=1):\n",
    "    sample_count = 0  # To track the total number of samples visualized\n",
    "    for batch_images, batch_targets in loader:\n",
    "        for idx in range(len(batch_images)):  # Loop through each sample in the batch\n",
    "            if sample_count >= num_samples:\n",
    "                return  # Stop visualization after reaching the required number of samples\n",
    "\n",
    "            # Access individual image and target\n",
    "            img_tensor = batch_images[idx]\n",
    "            if isinstance(img_tensor, list):  # Handle if it's a nested list\n",
    "                img_tensor = img_tensor[0]\n",
    "            \n",
    "            targets = batch_targets[idx]\n",
    "            if isinstance(targets, list):  # Handle if targets are a nested list\n",
    "                targets = targets[0]\n",
    "\n",
    "            # Handle different image channel formats\n",
    "            if img_tensor.shape[0] == 1:  # Grayscale\n",
    "                image = transforms.ToPILImage()(img_tensor.squeeze(0).cpu())\n",
    "            elif img_tensor.shape[0] == 3:  # RGB\n",
    "                image = transforms.ToPILImage()(img_tensor.cpu())\n",
    "            elif img_tensor.shape[0] == 4:  # RGBA\n",
    "                image = transforms.ToPILImage()(img_tensor[:3, :, :].cpu())  # Drop alpha\n",
    "            else:\n",
    "                print(f\"Unexpected number of channels: {img_tensor.shape[0]}. Skipping visualization.\")\n",
    "                continue\n",
    "\n",
    "            # Combine masks into a single 2D mask\n",
    "            masks = targets[\"masks\"].cpu().numpy()  # Shape: [N, H, W]\n",
    "            print(f\"Mask shape: {masks.shape}\")\n",
    "            combined_mask = np.sum(masks, axis=0)  # Sum across the first dimension (N)\n",
    "\n",
    "            # Clamp values to avoid unintended brightness if masks overlap\n",
    "            combined_mask = np.clip(combined_mask, 0, 1)\n",
    "\n",
    "            # Draw bounding boxes on the image\n",
    "            draw = ImageDraw.Draw(image)\n",
    "            boxes = targets[\"boxes\"].cpu().numpy()  # Shape: [N, 4]\n",
    "            for box in boxes:\n",
    "                if box.shape != (4,):  # Validate box shape\n",
    "                    print(f\"Invalid box format: {box}. Skipping.\")\n",
    "                    continue\n",
    "                # Ensure box coordinates are scalar values\n",
    "                x_min, y_min, x_max, y_max = [float(coord) for coord in box]\n",
    "                draw.rectangle([x_min, y_min, x_max, y_max], outline=\"yellow\", width=3)\n",
    "\n",
    "            # Visualize the image and combined mask\n",
    "            plt.figure(figsize=(10, 5))\n",
    "\n",
    "            # Image\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.imshow(image, cmap=\"gray\")\n",
    "            plt.title(f\"Image {sample_count + 1} with Bounding Boxes\")\n",
    "\n",
    "            # Combined mask\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.imshow(combined_mask, cmap=\"gray\")\n",
    "            plt.title(f\"Combined Mask {sample_count + 1} (Sum Across Channels)\")\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "            sample_count += 1  # Increment the sample counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ced095-5667-4ddb-8867-fe15fa9d7e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize examples from cropped datasets\n",
    "print(\"Visualizing Data:\")\n",
    "visualize_data(train_loader, num_samples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9922fb6-7d7d-42f8-95ab-daa5981834ce",
   "metadata": {},
   "source": [
    "# Part 4: fine-tune pre-trained model\n",
    "- Load the pre-trained model (maskrcnn_resnet50_fpn) from torchvision\n",
    "- Define the necessary training parameters (num_classes) and hyperparameters:\n",
    "    - learning rate\n",
    "    - weight decay\n",
    "    - scheduler's step_size and gamma\n",
    "    - epochs\n",
    "- Train model for the number of epochs defined. Note that the model's weights are saved after each epoch by default. This can use a lot of storage and might not be necessary in most cases.\n",
    "### IMPORTANT - If the notebook is to be used only to predict (without training), the pre-trained model still needs to be imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b19bd9-5e15-47d3-9104-f7c3f4ee972a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Load the pre-trained model\n",
    "image_height = 512  # Cropped image height\n",
    "image_width = 512   # Cropped image width\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(\n",
    "    min_size=image_height,\n",
    "    max_size=image_width,\n",
    "    pretrained=True\n",
    ")\n",
    "\n",
    "# Modify the model's head for dataset\n",
    "num_classes = 2  # Background + 1 ROI class\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Update the mask predictor\n",
    "in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "hidden_layer = 256\n",
    "model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "\n",
    "# Move the model to the device (GPU/CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Print the model to verify. Uncomment if needed\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ed38f0-3d2b-4048-8b86-428b95f0d81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_nms(outputs, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Apply Non-Maximum Suppression (NMS) to model predictions.\n",
    "\n",
    "    Args:\n",
    "        outputs (list of dict): List of predictions, each containing:\n",
    "                                - 'boxes': Tensor of shape [N, 4]\n",
    "                                - 'scores': Tensor of shape [N]\n",
    "                                - 'masks': Tensor of shape [N, H, W]\n",
    "        iou_threshold (float): IoU threshold for NMS.\n",
    "\n",
    "    Returns:\n",
    "        list of dict: Predictions after NMS.\n",
    "    \"\"\"\n",
    "    filtered_outputs = []\n",
    "    for output in outputs:\n",
    "        boxes = output['boxes']\n",
    "        scores = output['scores']\n",
    "        masks = output['masks']\n",
    "\n",
    "        # Apply NMS\n",
    "        keep_indices = nms(boxes, scores, iou_threshold)\n",
    "\n",
    "        # Filter predictions\n",
    "        filtered_outputs.append({\n",
    "            'boxes': boxes[keep_indices],\n",
    "            'scores': scores[keep_indices],\n",
    "            'masks': masks[keep_indices]\n",
    "        })\n",
    "\n",
    "    return filtered_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2048b204-b831-4ce9-83b4-daa6f0784f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This sets up the main training loop. We store the weights after each epoch for comparison purposes, but only the last one is needed.\n",
    "# To save space, \"model_save\" can be set to \"False\" (a checkpoint is ~300 Mb) \n",
    "\n",
    "# PARAMETERS (edit if needed)\n",
    "num_epochs = 25               # epochs per fold\n",
    "batch_size = 8\n",
    "num_workers = 0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "save_dir = models_dir         # reuse existing models_dir variable\n",
    "model_save = False            # save checkpoints\n",
    "save_best_only = False        # set to True to only save best val model per fold\n",
    "verbose = True\n",
    "\n",
    "# Get `data_list` from your dataset instance so we can extract groups (image_index)\n",
    "if hasattr(dataset, \"data\") and dataset.data is not None:\n",
    "    data_list = dataset.data\n",
    "elif hasattr(dataset, \"data_path\") and dataset.data_path:\n",
    "    data_list = torch.load(dataset.data_path)\n",
    "else:\n",
    "    raise RuntimeError(\"Dataset has no data loaded; please provide dataset.data or dataset.data_path\")\n",
    "\n",
    "#Build groups array: image_index must be present in crop metadata\n",
    "groups = []\n",
    "for i, d in enumerate(data_list):\n",
    "    meta = d.get(\"metadata\", {}) if isinstance(d, dict) else {}\n",
    "    img_idx = meta.get(\"image_index\", None)\n",
    "    if img_idx is None:\n",
    "        raise KeyError(f\"Crop at index {i} has no metadata/image_index.\")\n",
    "    groups.append(int(img_idx))\n",
    "\n",
    "# Make sure number of unique groups >= n_splits\n",
    "n_unique = len(set(groups))\n",
    "if n_splits > n_unique:\n",
    "    raise ValueError(f\"n_splits={n_splits} is larger than number of unique images ({n_unique}). Reduce n_splits.\")\n",
    "\n",
    "# Save base model state to restart from for each fold (important!)\n",
    "base_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "fold_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c05c318-ac59-4a5c-a466-f1da98a0c9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop.\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(splits):\n",
    "    print(f\"\\n=== Fold {fold_idx + 1}/{n_splits} ===\")\n",
    "    t0_fold = time.time()\n",
    "\n",
    "    # Restore base model weights\n",
    "    model.load_state_dict(base_state)\n",
    "    model.to(device)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Build Subset objects and DataLoaders\n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    train_loader_fold = DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        generator=g,  \n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=custom_collate_fn,\n",
    "        drop_last=False,)\n",
    "\n",
    "    val_loader_fold = None\n",
    "    if len(val_idx) > 0:  # Only create if validation exists\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "        val_loader_fold = DataLoader(\n",
    "            val_subset,\n",
    "            batch_size=batch_size//2,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=False,\n",
    "            collate_fn=custom_collate_fn,\n",
    "            drop_last=False,)\n",
    "\n",
    "    # Recreate optimizer & scheduler\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0001)\n",
    "    scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    history = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "    # Resume logic (skip val_loss if no validation)\n",
    "    existing_ckpts = [f for f in os.listdir(save_dir) if f.startswith(f\"fold{fold_idx+1}_epoch\")]\n",
    "    if existing_ckpts:\n",
    "        existing_ckpts.sort(key=lambda x: int(x.split(\"epoch\")[1].split(\".\")[0]))\n",
    "        latest_ckpt = existing_ckpts[-1]\n",
    "        ckpt_path = os.path.join(save_dir, latest_ckpt)\n",
    "        print(f\"Resuming from checkpoint {ckpt_path}\")\n",
    "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "        start_epoch = checkpoint[\"epoch\"]\n",
    "        history[\"train_loss\"] = [checkpoint.get(\"train_loss\", float(\"inf\"))]\n",
    "        if val_loader_fold is not None:\n",
    "            history[\"val_loss\"] = [checkpoint.get(\"val_loss\", float(\"inf\"))]\n",
    "    else:\n",
    "        print(\"No checkpoint found for this fold, starting fresh.\")\n",
    "        start_epoch = 0\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        print(f\" Fold {fold_idx+1} - Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for batch_idx, (images, targets) in enumerate(train_loader_fold):\n",
    "            if len(images) == 0 or len(targets) == 0:\n",
    "                continue\n",
    "\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k, v in t.items()}\n",
    "                       for t in targets]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            try:\n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "                losses.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "                optimizer.step()\n",
    "            except Exception as e:\n",
    "                print(f\"  Error during train batch {batch_idx+1}: {e}\")\n",
    "                continue\n",
    "\n",
    "            epoch_loss += losses.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        avg_train_loss = epoch_loss / num_batches if num_batches > 0 else float(\"inf\")\n",
    "        history[\"train_loss\"].append(avg_train_loss)\n",
    "        print(f\"  -> Epoch {epoch+1} train loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation (only if val_loader exists)\n",
    "        avg_val_loss = None\n",
    "        if val_loader_fold is not None:\n",
    "            torch.cuda.empty_cache()\n",
    "            model.eval()\n",
    "            val_loss_accum = 0.0\n",
    "            val_batches = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for images, targets in val_loader_fold:\n",
    "                    if len(images) == 0 or len(targets) == 0:\n",
    "                        continue\n",
    "\n",
    "                    images = [img.to(device) for img in images]\n",
    "                    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "                    model.train()\n",
    "                    try:\n",
    "                        loss_dict = model(images, targets)\n",
    "                        losses = sum(loss for loss in loss_dict.values())\n",
    "                    except Exception as e:\n",
    "                        print(f\"  Error during validation: {e}\")\n",
    "                        continue\n",
    "                    finally:\n",
    "                        model.eval()\n",
    "\n",
    "                    val_loss_accum += losses.item()\n",
    "                    val_batches += 1\n",
    "\n",
    "            avg_val_loss = val_loss_accum / val_batches if val_batches > 0 else float(\"inf\")\n",
    "            history[\"val_loss\"].append(avg_val_loss)\n",
    "            print(f\"  -> Epoch {epoch+1} val loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Step scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save checkpoint(s)\n",
    "        epoch_global = epoch + 1\n",
    "        if model_save:\n",
    "            chkname = f\"fold{fold_idx+1}_epoch{epoch_global}.pth\"\n",
    "            chkpath = os.path.join(save_dir, chkname)\n",
    "            torch.save({\n",
    "                \"fold\": fold_idx + 1,\n",
    "                \"epoch\": epoch_global,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                \"train_loss\": avg_train_loss,\n",
    "                \"val_loss\": avg_val_loss if avg_val_loss is not None else None,\n",
    "            }, chkpath)\n",
    "            # optionally keep only best\n",
    "            if save_best_only:\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    best_path = os.path.join(save_dir, f\"fold{fold_idx+1}_best.pth\")\n",
    "                    torch.save(model.state_dict(), best_path)\n",
    "        else:\n",
    "            # if not saving all, still save last or best\n",
    "            if epoch == (num_epochs - 1):\n",
    "                last_path = os.path.join(save_dir, f\"fold{fold_idx+1}_last.pth\")\n",
    "                torch.save(model.state_dict(), last_path)\n",
    "\n",
    "        # Save history to JSON after every epoch\n",
    "        hist_path = os.path.join(save_dir, f\"fold{fold_idx+1}_history.json\")\n",
    "        with open(hist_path, \"w\") as f:\n",
    "            json.dump(history, f)\n",
    "            \n",
    "    # fold finished\n",
    "    fold_time = time.time() - t0_fold\n",
    "    # Handle missing validation if k==1\n",
    "    if history[\"val_loss\"]:\n",
    "        last_val = history[\"val_loss\"][-1]\n",
    "        best_val = min(history[\"val_loss\"])\n",
    "        print(f\"Fold {fold_idx+1} finished in {fold_time/60:.2f} minutes. \"\n",
    "              f\"Last epoch val loss: {last_val:.4f}\")\n",
    "    else:\n",
    "        last_val = None\n",
    "        best_val = None\n",
    "        print(f\"Fold {fold_idx+1} finished in {fold_time/60:.2f} minutes. \"\n",
    "              f\"No validation performed for this fold.\")\n",
    "    \n",
    "    fold_results.append({\n",
    "        \"fold\": fold_idx + 1,\n",
    "        \"history\": history,\n",
    "        \"last_val_loss\": last_val,\n",
    "        \"best_val_loss\": best_val,\n",
    "    })\n",
    "    \n",
    "    # free GPU\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175b3361-2e21-4b4d-a5d1-194f820b0f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of folds\n",
    "# last_val and best_val will only be different right after training, not if you reload a final model\n",
    "\n",
    "for res in fold_results:\n",
    "    fold = res[\"fold\"]\n",
    "    last_val = res[\"last_val_loss\"]\n",
    "    best_val = res[\"best_val_loss\"]\n",
    "    if best_val is not None:\n",
    "        print(f\"Fold {fold}: last_val={last_val:.4f}, best_val={best_val:.4f}\")\n",
    "    else:\n",
    "        print(f\"Fold {fold}: no validation performed\")\n",
    "\n",
    "# Handle k = 1 and no validation case\n",
    "val_losses = [r[\"best_val_loss\"] for r in fold_results if r[\"best_val_loss\"] is not None]\n",
    "if val_losses:\n",
    "    mean_best = np.mean(val_losses)\n",
    "    print(f\"\\nGroupKFold summary: mean best val loss across folds = {mean_best:.4f}\")\n",
    "else:\n",
    "    print(\"\\nNo validation folds available — training completed on full dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2177c707-2fe9-486f-aed3-2608559cf744",
   "metadata": {},
   "source": [
    "### (FOR REPORTING ONLY, OPTIONAL) \n",
    "### Part 4.1: calculate loss vs epoch curves\n",
    "- Compares history files saved during training\n",
    "- Not needed if k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e82476-e89d-4fc6-a7a7-e8aa2ea701f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths. Folders were manually created to store different version of the training\n",
    "\n",
    "history_dir = r\"C:\\Users\\USER\\PROJECT\\Models\\Model\\History\"              # where fold histories are stored\n",
    "checkpoints_dir = r\"C:\\Users\\USER\\PROJECT\\Models\\Model\\Checkpoints\"      # where .pth files are stored\n",
    "best_output_dir = r\"C:\\Users\\USER\\PROJECT\\Models\\Model\\Best Models\"      # where to save cleaned best models\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "train_losses_all = []\n",
    "val_losses_all = []\n",
    "\n",
    "for fold in range(1, n_splits + 1):\n",
    "    # Load history json\n",
    "    history_path = os.path.join(history_dir, f\"fold{fold}_history.json\")\n",
    "    with open(history_path, \"r\") as f:\n",
    "        history = json.load(f)\n",
    "\n",
    "    train_losses = history[\"train_loss\"]\n",
    "    val_losses = history[\"val_loss\"]\n",
    "\n",
    "    train_losses_all.append(train_losses)\n",
    "    val_losses_all.append(val_losses)\n",
    "\n",
    "    # Find best epoch (lowest val loss)\n",
    "    best_epoch = int(np.argmin(val_losses))\n",
    "    best_val = val_losses[best_epoch]\n",
    "    print(f\"Fold {fold}: best epoch = {best_epoch+1}, val_loss = {best_val:.4f}\")\n",
    "\n",
    "    # Locate corresponding checkpoint\n",
    "    checkpoint_path = os.path.join(checkpoints_dir, f\"fold{fold}_epoch{best_epoch+1}.pth\")\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"!! Warning: checkpoint not found for fold {fold}, epoch {best_epoch+1}\")\n",
    "        continue\n",
    "\n",
    "    # Load and re-save clean version (only model state_dict)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    if \"model_state_dict\" in checkpoint:\n",
    "        model_state = checkpoint[\"model_state_dict\"]\n",
    "    else:\n",
    "        model_state = checkpoint  # assume already state_dict\n",
    "\n",
    "    save_path = os.path.join(best_output_dir, f\"fold{fold}_best.pth\")\n",
    "    #torch.save(model_state, save_path)\n",
    "    print(f\"Saved best model for fold {fold} at {save_path}\")\n",
    "\n",
    "\n",
    "# Plot mean ± std of train/val losses\n",
    "max_epochs = max(len(v) for v in val_losses_all)\n",
    "\n",
    "# Pad to same length for aggregation\n",
    "train_padded = [np.pad(v, (0, max_epochs - len(v)), constant_values=np.nan) for v in train_losses_all]\n",
    "val_padded   = [np.pad(v, (0, max_epochs - len(v)), constant_values=np.nan) for v in val_losses_all]\n",
    "\n",
    "train_array = np.vstack(train_padded)\n",
    "val_array = np.vstack(val_padded)\n",
    "\n",
    "train_mean = np.nanmean(train_array, axis=0)\n",
    "train_std  = np.nanstd(train_array, axis=0)\n",
    "val_mean   = np.nanmean(val_array, axis=0)\n",
    "val_std    = np.nanstd(val_array, axis=0)\n",
    "\n",
    "epochs = np.arange(1, max_epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(epochs, train_mean, label=\"Train loss (mean)\")\n",
    "plt.fill_between(epochs, train_mean-train_std, train_mean+train_std, alpha=0.2)\n",
    "plt.plot(epochs, val_mean, label=\"Val loss (mean)\")\n",
    "plt.fill_between(epochs, val_mean-val_std, val_mean+val_std, alpha=0.2)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Cross-validation loss curves\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "#plt.savefig(r\"C:\\Users\\USER\\PROJECT\\TRAINING\\ANALYSIS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5778c4-e8ec-49ac-b42e-9bdd02b450f8",
   "metadata": {},
   "source": [
    "### (OPTIONAL)\n",
    "### Part 4.2 : Re-load model\n",
    "- Once the model's weights are calculated from the training and stored, they can be quickly reloaded without going through training again.\n",
    "- If the notebook is is to be used only to predict (without training), also the pre-trained model needs to be re-loaded (see Part 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12518b5d-491d-4fc8-a295-38e137529263",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Re-load weights and check that all model's keys are matched\n",
    "weights_path = r\"C:\\Users\\USER\\PROJECT\\Models\\Model\\Checkpoints\\foldX_epochY.pth\" # Change to desired model to re-load\n",
    "state_dict = torch.load(weights_path, map_location=device, weights_only=True) \n",
    "model.load_state_dict(state_dict['model_state_dict'], strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a03354-852c-4a13-a187-bff438b0054d",
   "metadata": {},
   "source": [
    "# Part 5: Visualize predictions\n",
    "- Visualize predictions using cropped images in the test dataloader.\n",
    "- If the test images were not seen during training, the calculated metrics can be used for reporting.\n",
    "- Metrics can be calculated for different folds to determine their standard deviation\n",
    "- The same functions can now be used to analyze any data, including fresh new SEM images from the lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78918919-a8b3-4dfd-a65c-deb0dbc5d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the training data, also the test data only needs to be run and saved once. Uncomment on first run only\n",
    "\n",
    "#test_data = preprocess_data(test_image_dir, test_mask_dir, output_dir, crop_size=(512, 512), grid_size=(3, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62239b61-5081-4ee3-91d7-ecc552d8a500",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = os.path.join(output_dir,\"test_dataset_crops.pt\")\n",
    "test_dataset = ROIDataset(data_path=test_data_path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab60a5a-2f53-4534-8a37-0afcbf57ef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,   # keeps deterministic order for evaluation\n",
    "    collate_fn=custom_collate_fn,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df17aa2-4170-4053-8e8e-d6b34cde388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, device, iou_threshold=1, mask_threshold=0.7):\n",
    "    \"\"\"Compute quantitative metrics for one model and one loader.\"\"\"\n",
    "    model.eval()\n",
    "    ious, precisions, recalls, f1s, accuracies, aucs, log_losses = [], [], [], [], [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in loader:\n",
    "            if len(images) == 0 or len(targets) == 0:\n",
    "                continue\n",
    "\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = apply_nms(model(images), iou_threshold=iou_threshold)\n",
    "\n",
    "            for idx, output in enumerate(outputs):\n",
    "                pred_masks = output[\"masks\"].squeeze(1) > mask_threshold\n",
    "                combined_pred_mask = torch.any(pred_masks, dim=0).cpu().numpy()\n",
    "\n",
    "                true_masks = targets[idx][\"masks\"].cpu().numpy().astype(int)\n",
    "                combined_true_mask = np.any(true_masks, axis=0).astype(int)\n",
    "\n",
    "                # IoU\n",
    "                ious.append(jaccard_score(combined_true_mask.flatten(), combined_pred_mask.flatten()))\n",
    "\n",
    "                # Boxes → masks\n",
    "                pred_boxes = output[\"boxes\"].cpu().numpy()\n",
    "                true_boxes = targets[idx][\"boxes\"].cpu().numpy()\n",
    "                pred_box_mask = np.zeros_like(combined_true_mask, dtype=int)\n",
    "                true_box_mask = np.zeros_like(combined_true_mask, dtype=int)\n",
    "                for box in pred_boxes:\n",
    "                    x1, y1, x2, y2 = map(int, box)\n",
    "                    pred_box_mask[y1:y2, x1:x2] = 1\n",
    "                for box in true_boxes:\n",
    "                    x1, y1, x2, y2 = map(int, box)\n",
    "                    true_box_mask[y1:y2, x1:x2] = 1\n",
    "\n",
    "                precisions.append(precision_score(true_box_mask.flatten(), pred_box_mask.flatten(), zero_division=0))\n",
    "                recalls.append(recall_score(true_box_mask.flatten(), pred_box_mask.flatten(), zero_division=0))\n",
    "                f1s.append(f1_score(true_box_mask.flatten(), pred_box_mask.flatten(), zero_division=0))\n",
    "                accuracies.append(accuracy_score(true_box_mask.flatten(), pred_box_mask.flatten()))\n",
    "\n",
    "                # Probabilistic metrics\n",
    "                raw_pred_probs = output[\"masks\"].squeeze(1).cpu().numpy()\n",
    "                combined_prob_map = np.max(raw_pred_probs, axis=0) if raw_pred_probs.shape[0] > 0 else np.zeros_like(combined_true_mask)\n",
    "                combined_prob_map = np.clip(combined_prob_map, 1e-7, 1 - 1e-7)\n",
    "                flat_true = combined_true_mask.flatten()\n",
    "                flat_pred = combined_prob_map.flatten()\n",
    "\n",
    "                if np.any(flat_true) and np.any(flat_true == 0):\n",
    "                    try:\n",
    "                        aucs.append(roc_auc_score(flat_true, flat_pred))\n",
    "                        log_losses.append(log_loss(flat_true, flat_pred))\n",
    "                    except ValueError:\n",
    "                        aucs.append(np.nan)\n",
    "                        log_losses.append(np.nan)\n",
    "                else:\n",
    "                    aucs.append(np.nan)\n",
    "                    log_losses.append(np.nan)\n",
    "\n",
    "    return {\n",
    "        \"IoU\": np.array(ious),\n",
    "        \"Precision\": np.array(precisions),\n",
    "        \"Recall\": np.array(recalls),\n",
    "        \"F1\": np.array(f1s),\n",
    "        \"Accuracy\": np.array(accuracies),\n",
    "        \"AUC_ROC\": np.array(aucs),\n",
    "        \"LogLoss\": np.array(log_losses)\n",
    "    }\n",
    "\n",
    "def visualize_predictions(model, loader, device, mask_threshold=0.7, score_threshold=0.5):\n",
    "    \"\"\"Visualize predictions and masks for qualitative inspection.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, targets in loader:\n",
    "            if len(images) == 0 or len(targets) == 0:\n",
    "                continue\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = apply_nms(model(images), iou_threshold=1)\n",
    "\n",
    "            for idx, output in enumerate(outputs):\n",
    "                img_pil = F.to_pil_image(images[idx].cpu())\n",
    "                pred_masks = output[\"masks\"].squeeze(1) > mask_threshold\n",
    "                combined_pred_mask = torch.any(pred_masks, dim=0).cpu().numpy()\n",
    "                pred_boxes = output[\"boxes\"].cpu().numpy()\n",
    "                scores = output[\"scores\"].cpu().numpy()\n",
    "\n",
    "                fig, ax = plt.subplots(1, 2, figsize=(15, 7))\n",
    "                ax[0].imshow(img_pil, cmap=\"gray\")\n",
    "                ax[0].set_title(\"Original\")\n",
    "                ax[0].axis(\"off\")\n",
    "\n",
    "                ax[1].imshow(img_pil, alpha=0.8)\n",
    "                ax[1].imshow(combined_pred_mask, cmap=\"gray\", alpha=0.5)\n",
    "                for box, score in zip(pred_boxes, scores):\n",
    "                    if score > score_threshold:\n",
    "                        x1, y1, x2, y2 = box\n",
    "                        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                             edgecolor=\"red\", facecolor=\"none\", linewidth=2)\n",
    "                        ax[1].add_patch(rect)\n",
    "                        ax[1].text(x1, y1 - 5, f\"{score:.2f}\", color=\"red\",\n",
    "                                   bbox=dict(facecolor=\"white\", alpha=0.5, edgecolor=\"none\"))\n",
    "                ax[1].set_title(\"Predicted Masks + Boxes\")\n",
    "                ax[1].axis(\"off\")\n",
    "                plt.show()\n",
    "\n",
    "def evaluate_and_visualize(model, loader, device, best_output_dir=None, checkpoints_dir=None, history_dir=None,\n",
    "                           n_splits=1, visualize=False, visualize_best_fold=False):\n",
    "    \"\"\"\n",
    "    Unified evaluation:\n",
    "      - n_splits > 1: evaluate all folds and summarize.\n",
    "      - n_splits == 1: evaluate single model.\n",
    "    If visualize_best_fold=True (for k>1), visualize predictions from the best fold.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "\n",
    "    if n_splits > 1:\n",
    "        print(f\"\\nEvaluating {n_splits}-fold models...\\n\")\n",
    "        best_fold, best_val_loss = None, float(\"inf\")\n",
    "\n",
    "        for fold in range(1, n_splits + 1):\n",
    "            hist_path = os.path.join(history_dir, f\"fold{fold}_history.json\")\n",
    "            best_path = os.path.join(best_output_dir, f\"fold{fold}_best.pth\")\n",
    "            \n",
    "            # Fallback: if best model not found, use last checkpoint\n",
    "            if not os.path.exists(best_path):\n",
    "                ckpts = sorted(\n",
    "                    [f for f in os.listdir(checkpoints_dir) if f.startswith(f\"fold{fold}_epoch\")],\n",
    "                    key=lambda x: int(x.split(\"epoch\")[1].split(\".\")[0])\n",
    "                )\n",
    "                if ckpts:\n",
    "                    best_path = os.path.join(checkpoints_dir, ckpts[-1])            # 🔹 use checkpoints_dir for fallback\n",
    "                    print(f\"No best model found for fold {fold}, using last checkpoint: {ckpts[-1]}\")\n",
    "            \n",
    "            # Retrieve best validation loss if available\n",
    "            if os.path.exists(hist_path):\n",
    "                with open(hist_path, \"r\") as f:\n",
    "                    hist = json.load(f)\n",
    "                    val_losses = hist.get(\"val_loss\", [])\n",
    "                    if len(val_losses) > 0:\n",
    "                        min_val = min(val_losses)\n",
    "                        if min_val < best_val_loss:\n",
    "                            best_val_loss = min_val\n",
    "                            best_fold = fold\n",
    "\n",
    "            # Load model for metrics computation\n",
    "            model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "            model.to(device)\n",
    "            metrics = evaluate_model(model, loader, device)\n",
    "            all_results.append(metrics)\n",
    "\n",
    "        # Aggregate results\n",
    "        summary = {}\n",
    "        for key in all_results[0].keys():\n",
    "            values = [np.nanmean(res[key]) for res in all_results]\n",
    "            summary[key] = {\"mean\": np.nanmean(values), \"std\": np.nanstd(values)}\n",
    "\n",
    "        print(\"\\nCross-validation test metrics (mean ± std):\")\n",
    "        for metric, stats in summary.items():\n",
    "            print(f\"{metric}: {stats['mean']:.4f} ± {stats['std']:.4f}\")\n",
    "\n",
    "        # Visualize the best fold if requested\n",
    "        if visualize_best_fold and best_fold is not None:\n",
    "            print(f\"\\nBest fold: {best_fold} (val_loss={best_val_loss:.4f})\")\n",
    "            best_path = os.path.join(best_output_dir, f\"fold{best_fold}_best.pth\")\n",
    "            model.load_state_dict(torch.load(best_path, map_location=device, weights_only=True), strict=False)\n",
    "            model.to(device)\n",
    "            visualize_predictions(model, loader, device)\n",
    "\n",
    "        return summary\n",
    "\n",
    "    else:\n",
    "        print(\"\\nEvaluating single model (production mode)...\")\n",
    "        metrics = evaluate_model(model, loader, device)\n",
    "        print(\"\\nSingle-model metrics:\")\n",
    "        for k, v in metrics.items():\n",
    "            print(f\"{k}: {np.nanmean(v):.4f}\")\n",
    "\n",
    "        if visualize:\n",
    "            print(\"\\nVisualizing predictions...\")\n",
    "            visualize_predictions(model, loader, device)\n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09489302-3eaa-4bb7-8a24-52cf20860344",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if n_splits == 1:\n",
    "    metrics = evaluate_and_visualize(model, test_loader, device,\n",
    "                                     n_splits=n_splits,\n",
    "                                     visualize=True)\n",
    "elif n_splits > 1:\n",
    "    results_summary = evaluate_and_visualize(model, test_loader, device,\n",
    "                                             n_splits=n_splits,\n",
    "                                             visualize_best_fold=True,\n",
    "                                             best_output_dir=best_output_dir,\n",
    "                                             checkpoints_dir=checkpoints_dir,\n",
    "                                             history_dir=history_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a784b9-5e7a-43ee-a599-f769e08610a3",
   "metadata": {},
   "source": [
    "# Full-image predictions\n",
    "### - After loading a model (off-the-shelf or fine-tuned), this is the code actually used for predicting etch pits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffbc3b3-2f83-4668-a02d-0e47fe108d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full-image prediction via cropping and reconstruction\n",
    "# Pixel_size was previously determined and inserted manually in the metadata\n",
    "\n",
    "test_dir = r\"C:\\Users\\USER\\PROJECT\\TRAINING\\ANALYSIS\\Images-to-predict\"\n",
    "test_dir_pred = r\"C:\\Users\\USER\\PROJECT\\TRAINING\\ANALYSIS\\Images-to-predict\\Masks_predictions\"\n",
    "test_images_pred = r\"C:\\Users\\USER\\PROJECT\\TRAINING\\ANALYSIS\\Images-to-predict\\Images_predictions\"\n",
    "save_prediction_csv = False     # Save the pits data as CSV file \n",
    "save_prediction_images = False  # Save the final reconstructed image with overlaid predictions as a PNG file\n",
    "save_prediction_masks = False   # Save the summed prediction masks as a PNG file\n",
    "\n",
    "test_paths = sorted(glob.glob(os.path.join(test_dir, \"image_*\")))\n",
    "for test_image_path in test_paths:\n",
    "\n",
    "    # Load the test image\n",
    "    sum_final_boxes = []\n",
    "    sum_final_center_x = []\n",
    "    sum_final_center_y = []\n",
    "    sum_final_scores = []\n",
    "    sum_final_masks = []\n",
    "    sum_sides = []\n",
    "    measurements_unit = \"µm\"\n",
    "    test_image_idx = os.path.basename(test_image_path).split(\"_\")[1].split(\".\")[0]  # Extract 'i' from 'image_i'\n",
    "    test_image = Image.open(test_image_path).convert(\"I\")\n",
    "    x_res = test_image.info[\"resolution\"][0] # Check image metadata format\n",
    "    pixel_size_x = 1 / x_res\n",
    "    \n",
    "    # Convert to tensor and normalize\n",
    "    test_image_tensor = transforms.ToTensor()(test_image)\n",
    "    test_image_tensor = normalize_image(test_image_tensor)\n",
    "    \n",
    "    # Add batch dimension\n",
    "    test_image_tensor = test_image_tensor.unsqueeze(0).to(device)\n",
    "    \n",
    "    test_crops = split_image_and_targets(test_image_tensor[0],[],[],[], grid_size=(3, 5), crop_size=(512, 512))\n",
    "    \n",
    "    # Initialize blank canvas for reconstruction\n",
    "    H, W = test_image_tensor.shape[2:]\n",
    "    reconstructed_mask = np.zeros((H, W), dtype=np.float32)\n",
    "    all_boxes = []\n",
    "    all_scores = []\n",
    "    all_masks = []\n",
    "    \n",
    "    # Forward pass\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, crop in enumerate(test_crops):\n",
    "            # Prepare the crop\n",
    "            image = crop['image']\n",
    "            image = image.unsqueeze(0).to(device)  # Add batch and channel dimensions\n",
    "            y_start, x_start = crop['metadata']['y_start'], crop['metadata']['x_start']\n",
    "    \n",
    "            # Model predictions\n",
    "            test_outputs = model(image)\n",
    "            #test_outputs = apply_nms(test_outputs, iou_threshold=0.)\n",
    "    \n",
    "            # Extract predictions\n",
    "            output = test_outputs[0]\n",
    "            masks = output['masks'].squeeze(1).cpu().numpy() if 'masks' in output else []\n",
    "            boxes = output['boxes'].cpu().numpy() if 'boxes' in output else []\n",
    "            scores = output['scores'].cpu().numpy() if 'scores' in output else []\n",
    "    \n",
    "            # Store predictions for merging\n",
    "            for mask, box, score in zip(masks, boxes, scores):\n",
    "                if score > 0.5:  # Confidence threshold\n",
    "                    # Adjust bounding boxes to original coordinates\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    adjusted_box = [x1 + x_start, y1 + y_start, x2 + x_start, y2 + y_start]\n",
    "                    all_boxes.append(adjusted_box)\n",
    "                    all_scores.append(score)\n",
    "    \n",
    "                    # Place mask back in original coordinates\n",
    "                    adjusted_mask = np.zeros((H, W), dtype=np.float32)\n",
    "                    y_slice = slice(y_start, y_start + mask.shape[0])\n",
    "                    x_slice = slice(x_start, x_start + mask.shape[1])\n",
    "                    adjusted_mask[y_slice, x_slice] = mask\n",
    "                    all_masks.append(adjusted_mask)\n",
    "    \n",
    "    # Convert lists to tensors for NMS\n",
    "    all_boxes = torch.tensor(all_boxes, dtype=torch.float32)\n",
    "    all_scores = torch.tensor(all_scores, dtype=torch.float32)\n",
    "    if len(all_boxes) > 0:\n",
    "        # Apply NMS to filter overlapping boxes\n",
    "        keep_indices = nms(all_boxes, all_scores, iou_threshold=0.3)\n",
    "    \n",
    "        # Filter boxes, scores, and masks based on NMS results\n",
    "        final_boxes = all_boxes[keep_indices].numpy()\n",
    "        final_scores = all_scores[keep_indices].numpy()\n",
    "        final_masks = [all_masks[i] for i in keep_indices]\n",
    "    \n",
    "        # Combine masks into the reconstructed image\n",
    "        for mask in final_masks:\n",
    "            reconstructed_mask = np.maximum(reconstructed_mask, mask)\n",
    "    \n",
    "        # Plot summed predicted masks\n",
    "        if save_prediction_masks == True:\n",
    "            plt.imsave((os.path.join(test_dir_pred,f\"image_{test_image_idx}.png\")), reconstructed_mask, cmap='gray')\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(test_image_tensor.cpu().squeeze(), cmap='gray', alpha=0.5)\n",
    "        plt.imshow(reconstructed_mask, cmap='gray', alpha=0.5)\n",
    "        \n",
    "        for box in final_boxes:\n",
    "            x1, y1, x2, y2 = box\n",
    "            center_x = x1 + (x2 - x1)/2\n",
    "            center_y = y1 + (y2 - y1)/2 \n",
    "            rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, edgecolor='red', facecolor='none', linewidth=2)\n",
    "            plt.gca().add_patch(rect)\n",
    "            area = 0.5 * (x2 - x1) * (y2 - y1) * (pixel_size_x**2)\n",
    "            size = np.sqrt((4/np.sqrt(3))*area)\n",
    "            sum_final_boxes.append(box)\n",
    "            sum_final_center_x.append(center_x)\n",
    "            sum_final_center_y.append(center_y)\n",
    "            sum_sides.append(size)\n",
    "        \n",
    "        # Plot the reconstructed images with overlaid predictions. Plot pits size histograms\n",
    "        sum_dict = {'x':sum_final_center_x, 'y':sum_final_center_y, 'side': sum_sides}\n",
    "        df = pd.DataFrame(sum_dict) \n",
    "        if save_prediction_csv == True:\n",
    "            save_path_data = os.path.join(test_images_pred, f\"image_{test_image_idx}.csv\")\n",
    "            df.to_csv(save_path_data)\n",
    "        plt.axis('off')  # Turn off the axis for better visualization\n",
    "        if save_prediction_images == True:\n",
    "            save_path_img = os.path.join(test_images_pred, f\"image_{test_image_idx}.png\")\n",
    "            plt.savefig(save_path_img, bbox_inches='tight', pad_inches=0)                                    \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(sum_sides, bins=np.arange(0, 0.45 + 0.02, 0.02), color='blue', alpha=0.7, edgecolor='black', weights=np.ones_like(sum_sides) / len(sum_sides))\n",
    "        plt.title(\"Histogram of image \"+str(test_image_idx), fontsize=16)\n",
    "        plt.xlabel(\"Size (µm)\", fontsize=14)\n",
    "        plt.ylabel(\"Relative frequency\", fontsize=14)\n",
    "        ax = plt.gca()\n",
    "        ax.set_xlim([0, 0.45])\n",
    "        ax.set_ylim([0, 0.45])\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9da856-dffb-48de-918b-f437cc258762",
   "metadata": {},
   "source": [
    "### End of notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
