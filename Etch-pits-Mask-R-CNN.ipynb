{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4784eac2-cdb7-4bb7-b550-c6cb819c6375",
   "metadata": {},
   "source": [
    "# Part 1: Imports\n",
    "- Import the necessary libraries for training and analysis, and define the paths to the images/masks folders to be used for training.\n",
    "- The naming convention is \"image_i\" and \"image_i_mask_j\".\n",
    "- Images used for training are in the .png format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a43040-c710-49ef-ac77-c3641a47ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# Uncomment last line to install PyTorch with CUDA (check compatible version) \n",
    "# If there are conflicting dependencies when installing with Anaconda, it can be installed directly from the notebook\n",
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507de43e-3546-4148-9817-48d676bfff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#Imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision # Pre-trained Mask R-CNN model\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import to_pil_image, to_tensor\n",
    "from torchvision.ops import nms\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import tifffile\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, jaccard_score\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1936a2-4fc3-4c61-baca-41c8921a1cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check that GPU is available for calculations, and set the \"device\" variable\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n",
    "# Check if GPU is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device: \",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0669020-ed3d-4c18-b546-18874cbb4fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to image and masks for training\n",
    "image_dir = r\"C:\\Users\\leona\\0_etch pits_analysis\\etch pits test\\Training\\Images\"\n",
    "mask_dir = r\"C:\\Users\\leona\\0_etch pits_analysis\\etch pits test\\Training\\Masks\"\n",
    "output_dir = r\"C:\\Users\\leona\\0_etch pits_analysis\\etch pits test\\Pre-process\"\n",
    "models_dir = r\"C:\\Users\\leona\\0_etch pits_analysis\\etch pits test\\Models\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b33916b-5811-4ffb-85b8-f157a46b81f5",
   "metadata": {},
   "source": [
    "# Part 2: Data pre-processing\n",
    "- Define the pre-processing of the data that will be used for the training of the model.\n",
    "- The pre-processing consists of:\n",
    "    - load the data into the correct shape for training. Some steps require the channel dimension or a dummy dimension to work; \n",
    "    - normalizing image values to the 0-1 range;\n",
    "    - splitting the image in overlapping squares (512x512 size) using a grid (5x3), maintaning the link with the corresponding masks;\n",
    "    - removing training data that is not fully in a crop; \n",
    "    - applying augmentation functions to improve generalization of the model.\n",
    "- The data is saved as a .pt file to free up memory (large training datasets can consume a lot of memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894025fb-4970-47bb-9d3e-b6fdacb2dbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(image_dir, mask_dirs, output_dir, crop_size=(512, 512), grid_size=(2, 3), augment=False):\n",
    "    \"\"\"\n",
    "    Preprocess images and masks by cropping and saving them using a grid.\n",
    "\n",
    "    Args:\n",
    "        image_dir (list of str): List of paths to input images.\n",
    "        mask_dirs (list of str): List of directories containing masks for each image.\n",
    "        output_dir (str): Directory to save preprocessed data.\n",
    "        crop_size (tuple): Size of the crops (H, W).\n",
    "        grid_size (tuple): Grid dimensions for cropping (rows, cols).\n",
    "        augment (bool): Presence/absence of augmentation function\n",
    "    \"\"\"\n",
    "    dataset = []  # List to hold all samples\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    crop_H, crop_W = crop_size\n",
    "\n",
    "    # List all images in the image directory\n",
    "    image_paths = sorted(glob.glob(os.path.join(image_dir, \"image_*\")))  # Match image_i format\n",
    "    \n",
    "    for image_path in image_paths:\n",
    "        # Extract image index\n",
    "        image_idx = os.path.basename(image_path).split(\"_\")[1].split(\".\")[0]  # Extract 'i' from 'image_i'\n",
    "        \n",
    "        # Match corresponding masks for the image\n",
    "        mask_pattern = os.path.join(mask_dir, f\"image_{image_idx}_mask_*\")\n",
    "        mask_files = sorted(glob.glob(mask_pattern))  # Match image_i_mask_j format\n",
    "\n",
    "        # Load the image\n",
    "        image = Image.open(image_path).convert(\"I\")\n",
    "        image_tensor = transforms.ToTensor()(image)  # Convert to tensor [C, H, W]\n",
    "\n",
    "        # Load masks\n",
    "        masks = []\n",
    "        for mask_path in mask_files:\n",
    "            mask = Image.open(mask_path).convert('L')\n",
    "            masks.append(np.array(mask, dtype=np.uint8))\n",
    "        masks = np.stack(masks, axis=0)  # Shape: [N, H, W]\n",
    "        masks_tensor = torch.tensor(masks, dtype=torch.uint8)  # [N, H, W]\n",
    "\n",
    "        # Generate dummy boxes for all masks\n",
    "        boxes = []\n",
    "        for mask in masks:\n",
    "            pos = np.where(mask > 0)\n",
    "            if pos[0].size > 0:  # Check for non-empty mask\n",
    "                xmin, ymin, xmax, ymax = np.min(pos[1]), np.min(pos[0]), np.max(pos[1]), np.max(pos[0])\n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "        boxes_tensor = torch.tensor(boxes, dtype=torch.float32)  # [N, 4]\n",
    "\n",
    "        # Assign dummy labels (1 for all instances, can be adjusted later)\n",
    "        labels_tensor = torch.ones((len(boxes),), dtype=torch.int64)  # [N]\n",
    "\n",
    "        # Split into crops\n",
    "        crops = split_image_and_targets(\n",
    "            image=image_tensor,\n",
    "            masks=masks_tensor,\n",
    "            boxes=boxes_tensor,\n",
    "            labels=labels_tensor,\n",
    "            grid_size=grid_size,\n",
    "            crop_size=crop_size,\n",
    "            augment=augment,\n",
    "            image_index=image_idx\n",
    "        )\n",
    "        # Append all crops to the dataset\n",
    "        dataset.extend(crops)\n",
    "        \n",
    "    # Save as .pt file\n",
    "    crop_filename = \"dataset_crops.pt\"\n",
    "    torch.save(dataset, os.path.join(output_dir, crop_filename))    \n",
    "\n",
    "    print(f\"Processed images -> Saved crops to {crop_filename}\")\n",
    "  \n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea8719b-1eb5-4813-a54e-8f44459b4e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define augmentations\n",
    "augmentation_pipeline = T.Compose([\n",
    "    T.RandomHorizontalFlip(p=0.3),\n",
    "    T.RandomVerticalFlip(p=0.3),\n",
    "    T.RandomRotation(degrees=30),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    T.RandomResizedCrop(size=(512, 512), scale=(0.2, 1)),\n",
    "    T.GaussianBlur(kernel_size=(1, 25), sigma=(10., 15.)),\n",
    "    T.RandomPerspective(distortion_scale=0.2, p=0.35)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bdfbfc-3b73-4394-8fc5-53acc8eaea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_augmentation(crop_data):\n",
    "    \"\"\"\n",
    "    Apply augmentations to a crop (image, masks, boxes).\n",
    "\n",
    "    Args:\n",
    "        crop_data (dict): Contains 'image', 'masks', 'boxes', 'labels'.\n",
    "\n",
    "    Returns:\n",
    "        dict: Augmented crop data.\n",
    "    \"\"\"\n",
    "    image = to_pil_image(crop_data['image'])  # Convert tensor to PIL\n",
    "    augmented_image = augmentation_pipeline(image)  # Apply augmentations\n",
    "    crop_data['image'] = to_tensor(augmented_image)  # Convert back to tensor\n",
    "    return crop_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f197cc-4efe-48ac-b850-58ac050cccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image_and_targets(image, masks, boxes, labels, grid_size=(2, 3), crop_size=(512, 512), augment=False, image_index=0):\n",
    "    \"\"\"\n",
    "    Splits the image, masks, and bounding boxes into overlapping crops with optional augmentations.\n",
    "\n",
    "    Args:\n",
    "        image (Tensor): Original image tensor of shape [C, H, W].\n",
    "        masks (Tensor): Masks tensor of shape [N, H, W].\n",
    "        boxes (Tensor): Bounding boxes tensor of shape [N, 4].\n",
    "        labels (Tensor): Labels tensor of shape [N].\n",
    "        grid_size (tuple): (rows, cols) for splitting the image.\n",
    "        crop_size (tuple): (height, width) of each crop.\n",
    "        augment (bool): Whether to apply augmentations.\n",
    "        image_index (int): Index of the image in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing:\n",
    "              - 'image': Cropped image tensor [C, crop_H, crop_W].\n",
    "              - 'masks': Cropped masks tensor [N', crop_H, crop_W].\n",
    "              - 'boxes': Adjusted bounding boxes tensor [N', 4].\n",
    "              - 'labels': Cropped labels tensor [N'].\n",
    "              - 'metadata': Metadata dictionary containing:\n",
    "                            - 'image_index': Index of the original image.\n",
    "                            - 'x_start': x-coordinate of the crop's top-left corner.\n",
    "                            - 'y_start': y-coordinate of the crop's top-left corner.\n",
    "    \"\"\"\n",
    "    H, W = image.shape[1], image.shape[2]\n",
    "    crop_H, crop_W = crop_size\n",
    "    rows, cols = grid_size\n",
    "\n",
    "    stride_H = (H - crop_H) // (rows - 1) if rows > 1 else 0\n",
    "    stride_W = (W - crop_W) // (cols - 1) if cols > 1 else 0\n",
    "\n",
    "    crops = []\n",
    "\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            # Compute crop coordinates\n",
    "            y_start = min(row * stride_H, H - crop_H)\n",
    "            x_start = min(col * stride_W, W - crop_W)\n",
    "            y_end = y_start + crop_H\n",
    "            x_end = x_start + crop_W\n",
    "\n",
    "            # Crop the image\n",
    "            cropped_image = image[:, y_start:y_end, x_start:x_end]\n",
    "\n",
    "            # Adjust bounding boxes\n",
    "            cropped_masks = []\n",
    "            cropped_boxes = []\n",
    "            cropped_labels = []\n",
    "\n",
    "            for mask_idx, box in enumerate(boxes):\n",
    "                x_min, y_min, x_max, y_max = box\n",
    "                # Check if the box overlaps with the crop\n",
    "                if x_min < x_end and x_max > x_start and y_min < y_end and y_max > y_start:\n",
    "                    # Adjust box coordinates to crop-relative\n",
    "                    new_x_min = max(x_min - x_start, 0)\n",
    "                    new_y_min = max(y_min - y_start, 0)\n",
    "                    new_x_max = min(x_max - x_start, crop_W)\n",
    "                    new_y_max = min(y_max - y_start, crop_H)\n",
    "                    cropped_boxes.append([new_x_min, new_y_min, new_x_max, new_y_max])\n",
    "                    cropped_labels.append(labels[mask_idx].item())  # Convert label to int\n",
    "                    \n",
    "                    # Crop the corresponding mask\n",
    "                    cropped_mask = masks[mask_idx, y_start:y_end, x_start:x_end]\n",
    "                    cropped_mask = (cropped_mask > 0).float()\n",
    "                    cropped_masks.append(cropped_mask)\n",
    "\n",
    "            # Stack masks into a tensor if there are valid masks\n",
    "            if cropped_masks:\n",
    "                cropped_masks = torch.stack(cropped_masks, dim=0)\n",
    "            else:\n",
    "                cropped_masks = torch.zeros((0, crop_H, crop_W), dtype=torch.uint8)\n",
    "                \n",
    "            # Add metadata to the crop for reconstruction\n",
    "            metadata = {\n",
    "                \"image_index\": image_index,\n",
    "                \"x_start\": x_start,\n",
    "                \"y_start\": y_start,\n",
    "            }\n",
    "        \n",
    "            # Add to results if there are valid boxes\n",
    "            crop_data = {\n",
    "                    \"image\": cropped_image,\n",
    "                    \"masks\": cropped_masks,\n",
    "                    \"boxes\": torch.tensor(cropped_boxes, dtype=torch.float32),\n",
    "                    \"labels\": torch.tensor(cropped_labels, dtype=torch.int64),\n",
    "                    \"metadata\": metadata\n",
    "                }\n",
    "                \n",
    "            if augment:\n",
    "                crop_data = apply_augmentation(crop_data)\n",
    "\n",
    "            crops.append(crop_data)\n",
    "\n",
    "    return crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a75cf0-92e2-4792-a94b-e4e926c1b328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image(image, percentile_min=2, percentile_max=98, padding_value=0):\n",
    "    \"\"\"\n",
    "    Normalize the image while handling outliers and ignoring padded edges.\n",
    "    \n",
    "    Args:\n",
    "        image (torch.Tensor): Input image tensor.\n",
    "        percentile_min (float): Lower percentile for clipping (default: 2%).\n",
    "        percentile_max (float): Upper percentile for clipping (default: 98%).\n",
    "        padding_value (float): Value of padded background (default: 0).\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Normalized image tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Mask out padded regions\n",
    "    pad_mask = image != padding_value\n",
    "    if not pad_mask.any():\n",
    "        print(\"Warning: Entire image is padding. Returning zeros.\")\n",
    "        return torch.zeros_like(image)\n",
    "        \n",
    "    # Ensure the image is a float tensor\n",
    "    image = image.float()\n",
    "\n",
    "    # Extract valid pixel values (ignoring padding)\n",
    "    valid_pixels = image[pad_mask]\n",
    "    \n",
    "    # Compute percentile-based clipping bounds\n",
    "    min_val = torch.quantile(valid_pixels, percentile_min / 100.0)\n",
    "    max_val = torch.quantile(valid_pixels, percentile_max / 100.0)\n",
    "    \n",
    "    # Clip values to the specified percentiles\n",
    "    clipped_image = torch.clip(image, min=min_val, max=max_val)\n",
    "    \n",
    "    # Normalize to [0, 1], ignoring padding\n",
    "    normalized_image = (clipped_image - min_val) / (max_val - min_val)\n",
    "    normalized_image[~pad_mask] = 0  # Restore padding values to 0    \n",
    "    \n",
    "    # Avoid division by zero if all values are the same\n",
    "    if max_val == min_val:\n",
    "        print(\"Warning: Image has uniform intensity. Returning zeros.\")\n",
    "        return torch.zeros_like(image)\n",
    "\n",
    "    return normalized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37219aae-0776-4266-bd71-95a700f87e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can handle both data in memory or saved as a .pt file\n",
    "class ROIDataset(Dataset):\n",
    "    def __init__(self, dataset=None, data_path=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset (list or None): Preloaded dataset (list of dictionaries with 'image', 'masks', etc.).\n",
    "            data_path (str or None): Directory containing preprocessed `.pt` files.\n",
    "            transform (callable, optional): Optional transform to be applied to the image.\n",
    "        \"\"\"\n",
    "        if data_path:\n",
    "            self.data_path = data_path\n",
    "            self.data = torch.load(self.data_path)  # Data will be loaded from file\n",
    "        elif dataset:\n",
    "            self.data = dataset\n",
    "            self.data_path = None # No files to load\n",
    "        else:\n",
    "            raise ValueError(\"Either 'dataset' or 'data_path' must be provided.\")\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    @staticmethod\n",
    "    def is_mask_touching_border(mask, threshold=0, threshold_size=0):\n",
    "        \"\"\"\n",
    "        Check if a mask touches the border of an image for more than a threshold of pixels.\n",
    "        \"\"\"\n",
    "        \n",
    "        h, w = mask.shape\n",
    "        top_border = mask[0, :].sum()\n",
    "        bottom_border = mask[-1, :].sum()\n",
    "        left_border = mask[:, 0].sum()\n",
    "        right_border = mask[:, -1].sum()\n",
    "        mask_size = h*w\n",
    "        return (top_border > threshold or \n",
    "                bottom_border > threshold or \n",
    "                left_border > threshold or \n",
    "                right_border > threshold or\n",
    "                mask_size < threshold_size)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if self.data_path:\n",
    "            # Load preprocessed data from file \n",
    "            data = self.data[idx]\n",
    "        elif self.data:\n",
    "            # Use in-memory dataset\n",
    "            data = self.data[idx]\n",
    "        else:\n",
    "            raise RuntimeError(\"Dataset is not properly initialized.\")\n",
    "\n",
    "        image = data[\"image\"]\n",
    "        if self.transform:\n",
    "            if isinstance(image, torch.Tensor):\n",
    "                image = transforms.ToPILImage()(image)  # Convert back to PIL for transforms\n",
    "            image = self.transform(image)\n",
    "        image = normalize_image(image)\n",
    "        \n",
    "        # Filter masks that touch the border\n",
    "        masks = data[\"masks\"]\n",
    "        boxes = data[\"boxes\"]\n",
    "        labels = data[\"labels\"]\n",
    "            \n",
    "        valid_indices = []\n",
    "        for i, mask in enumerate(masks):\n",
    "            if not self.is_mask_touching_border(mask.numpy(), threshold=5, threshold_size=30):\n",
    "                valid_indices.append(i)\n",
    "                \n",
    "        # Apply filtering\n",
    "        masks = masks[valid_indices]\n",
    "        boxes = boxes[valid_indices]\n",
    "        labels = labels[valid_indices]\n",
    "        \n",
    "        # Skip if no valid masks remain\n",
    "        if len(valid_indices) == 0:\n",
    "            return None  # Indicate this crop should be skipped\n",
    "        \n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"masks\": masks,\n",
    "        }\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caf3f5f-ee43-4c82-a167-1565f0e08c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After having defined all the functions, the lines below uses them\n",
    "data = preprocess_data(image_dir, mask_dir, output_dir, crop_size=(512, 512), grid_size=(3, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fe0356-b828-45a7-a57e-b6738fa8ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "# We use Compose in case we want to add pre-processing steps later on\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create uncropped dataset and dataloader\n",
    "data_path = os.path.join(output_dir,\"dataset_crops.pt\")\n",
    "dataset = ROIDataset(data_path=data_path, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0f258f-1b42-4ec4-994f-8ff063c2fe85",
   "metadata": {},
   "source": [
    "# Part 3: Build dataloaders\n",
    "- With the input data in the correct shape, we can now build the training dataset in a way that the model can use it for training.\n",
    "- The data is split in training, validation and test datasets.\n",
    "- A few examples are visualized to check that the correct data is being loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99283d3-ca6a-42c1-abe1-c4972ce9164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    # Remove None entries\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    \n",
    "    # If batch is empty, return empty lists (skip this batch during training)\n",
    "    if len(batch) == 0:\n",
    "        return [], []\n",
    "    \n",
    "    images, targets = zip(*batch)\n",
    "    return list(images), list(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0738b0-8357-4216-a2d7-6fde3eb78a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset sizes\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.80 * dataset_size)  # 80% for training\n",
    "val_size = int(0.12 * dataset_size)   # 12% for validation\n",
    "test_size = dataset_size - train_size - val_size  # Remaining 8% for testing\n",
    "\n",
    "# Split dataset\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Create dataloaders for each split\n",
    "batch_size = 8  # Define batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, pin_memory=True, shuffle=True, collate_fn=custom_collate_fn, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, pin_memory=True, shuffle=False, collate_fn=custom_collate_fn, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, pin_memory=True, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd055632-3f0b-40ac-844b-cacf219ed733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization for both uncropped and cropped data\n",
    "def visualize_data(loader, num_samples=1):\n",
    "    sample_count = 0  # To track the total number of samples visualized\n",
    "    for batch_images, batch_targets in loader:\n",
    "        for idx in range(len(batch_images)):  # Loop through each sample in the batch\n",
    "            if sample_count >= num_samples:\n",
    "                return  # Stop visualization after reaching the required number of samples\n",
    "\n",
    "            # Access individual image and target\n",
    "            img_tensor = batch_images[idx]\n",
    "            if isinstance(img_tensor, list):  # Handle if it's a nested list\n",
    "                img_tensor = img_tensor[0]\n",
    "            \n",
    "            targets = batch_targets[idx]\n",
    "            if isinstance(targets, list):  # Handle if targets are a nested list\n",
    "                targets = targets[0]\n",
    "\n",
    "            # Handle different image channel formats\n",
    "            if img_tensor.shape[0] == 1:  # Grayscale\n",
    "                image = transforms.ToPILImage()(img_tensor.squeeze(0).cpu())\n",
    "            elif img_tensor.shape[0] == 3:  # RGB\n",
    "                image = transforms.ToPILImage()(img_tensor.cpu())\n",
    "            elif img_tensor.shape[0] == 4:  # RGBA\n",
    "                image = transforms.ToPILImage()(img_tensor[:3, :, :].cpu())  # Drop alpha\n",
    "            else:\n",
    "                print(f\"Unexpected number of channels: {img_tensor.shape[0]}. Skipping visualization.\")\n",
    "                continue\n",
    "\n",
    "            # Combine masks into a single 2D mask\n",
    "            masks = targets[\"masks\"].cpu().numpy()  # Shape: [N, H, W]\n",
    "            print(f\"Mask shape: {masks.shape}\")\n",
    "            combined_mask = np.sum(masks, axis=0)  # Sum across the first dimension (N)\n",
    "\n",
    "            # Clamp values to avoid unintended brightness if masks overlap\n",
    "            combined_mask = np.clip(combined_mask, 0, 1)\n",
    "\n",
    "            # Draw bounding boxes on the image\n",
    "            draw = ImageDraw.Draw(image)\n",
    "            boxes = targets[\"boxes\"].cpu().numpy()  # Shape: [N, 4]\n",
    "            for box in boxes:\n",
    "                if box.shape != (4,):  # Validate box shape\n",
    "                    print(f\"Invalid box format: {box}. Skipping.\")\n",
    "                    continue\n",
    "                # Ensure box coordinates are scalar values\n",
    "                x_min, y_min, x_max, y_max = [float(coord) for coord in box]\n",
    "                draw.rectangle([x_min, y_min, x_max, y_max], outline=\"yellow\", width=3)\n",
    "\n",
    "            # Visualize the image and combined mask\n",
    "            plt.figure(figsize=(10, 5))\n",
    "\n",
    "            # Image\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.imshow(image, cmap=\"gray\")\n",
    "            plt.title(f\"Image {sample_count + 1} with Bounding Boxes\")\n",
    "\n",
    "            # Combined mask\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.imshow(combined_mask, cmap=\"gray\")\n",
    "            plt.title(f\"Combined Mask {sample_count + 1} (Sum Across Channels)\")\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "            sample_count += 1  # Increment the sample counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ced095-5667-4ddb-8867-fe15fa9d7e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize examples from cropped datasets\n",
    "print(\"Visualizing Data:\")\n",
    "visualize_data(train_loader, num_samples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9922fb6-7d7d-42f8-95ab-daa5981834ce",
   "metadata": {},
   "source": [
    "# Part 4: fine-tune pre-trained model\n",
    "- Load the pre-trained model (maskrcnn_resnet50_fpn) from torchvision\n",
    "- Define the necessary training parameters (num_classes) and hyperparameters:\n",
    "    - learning rate\n",
    "    - weight decay\n",
    "    - scheduler's step_size and gamma\n",
    "    - epochs\n",
    "- Train model for the number of epochs defined. Note that the model's weights are saved after each epoch by default. This can use a lot of storage and might not be necessary in most cases.\n",
    "### IMPORTANT - If the notebook is to be used only to predict (without training), the pre-trained model still needs to be imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b19bd9-5e15-47d3-9104-f7c3f4ee972a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Load the pre-trained model\n",
    "image_height = 512  # Uncropped image height\n",
    "image_width = 512   # Uncropped image width\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(\n",
    "    min_size=image_height,\n",
    "    max_size=image_width,\n",
    "    pretrained=True\n",
    ")\n",
    "\n",
    "# Modify the model's head for dataset\n",
    "num_classes = 2  # Background + 1 ROI class\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Update the mask predictor\n",
    "in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "hidden_layer = 256\n",
    "model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "\n",
    "# Move the model to the device (GPU/CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Print the model to verify. Uncomment if needed\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8759674-fd36-4119-af5d-26ea5cb41662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0001) # Hyperparameters1\n",
    "\n",
    "# Define learning rate scheduler\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)  # Hyperparameters2: Decays lr by gamma every 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ed38f0-3d2b-4048-8b86-428b95f0d81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_nms(outputs, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Apply Non-Maximum Suppression (NMS) to model predictions.\n",
    "\n",
    "    Args:\n",
    "        outputs (list of dict): List of predictions, each containing:\n",
    "                                - 'boxes': Tensor of shape [N, 4]\n",
    "                                - 'scores': Tensor of shape [N]\n",
    "                                - 'masks': Tensor of shape [N, H, W]\n",
    "        iou_threshold (float): IoU threshold for NMS.\n",
    "\n",
    "    Returns:\n",
    "        list of dict: Predictions after NMS.\n",
    "    \"\"\"\n",
    "    filtered_outputs = []\n",
    "    for output in outputs:\n",
    "        boxes = output['boxes']\n",
    "        scores = output['scores']\n",
    "        masks = output['masks']\n",
    "\n",
    "        # Apply NMS\n",
    "        keep_indices = nms(boxes, scores, iou_threshold)\n",
    "\n",
    "        # Filter predictions\n",
    "        filtered_outputs.append({\n",
    "            'boxes': boxes[keep_indices],\n",
    "            'scores': scores[keep_indices],\n",
    "            'masks': masks[keep_indices]\n",
    "        })\n",
    "\n",
    "    return filtered_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c05c318-ac59-4a5c-a466-f1da98a0c9b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is the main training loop. We store the weights after each epoch for comparison purposes, but only the last one is needed.\n",
    "# To save space, \"model_save\" can be set to \"False\".\n",
    "\n",
    "model_save = True\n",
    "num_epochs = 30\n",
    "training_losses = []  # List to store average loss per epoch\n",
    "validation_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    epoch_loss = 0  # Accumulate loss for the epoch\n",
    "    num_batches = 0  # Count batches in the epoch\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "        # Skip empty batches\n",
    "        if len(images) == 0 or len(targets) == 0:\n",
    "            print(\"Skipping empty batch...\")\n",
    "            continue\n",
    "\n",
    "        # Move data to device\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [\n",
    "            {\n",
    "                k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n",
    "                for k, v in t.items()\n",
    "            }\n",
    "            for t in targets\n",
    "        ]\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        try:\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values()) # Compute total loss\n",
    "            losses.backward() # Backward pass\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0) # Gradient clipping\n",
    "            optimizer.step() # Update parameters\n",
    "        except Exception as e:\n",
    "            print(f\"Error during forward pass: {e}\")\n",
    "            continue\n",
    "            \n",
    "        # Update loss trackers\n",
    "        epoch_loss += losses.item()\n",
    "        num_batches += 1\n",
    "        print(f\"  Batch {batch_idx + 1}: Loss = {losses.item():.4f}\")\n",
    "\n",
    "    # Compute average training loss\n",
    "    avg_train_loss = epoch_loss / num_batches if num_batches > 0 else float('inf')\n",
    "    training_losses.append(avg_train_loss)\n",
    "    print(f\"Epoch {epoch + 1} Training Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0\n",
    "    val_batches = 0\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Validation Phase\")\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch_idx, (images, targets) in enumerate(val_loader):  # Use validation dataloader\n",
    "    \n",
    "            # Move data to device\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [\n",
    "                {\n",
    "                    k: (v.to(device) if isinstance(v, torch.Tensor) else v)\n",
    "                    for k, v in t.items()\n",
    "                }\n",
    "                for t in targets\n",
    "            ]\n",
    "            # Temporarily switch to training mode for loss calculation\n",
    "            model.train()\n",
    "            try:\n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "                val_loss += losses.item()\n",
    "                val_batches += 1\n",
    "                print(f\"  Validation Batch {batch_idx + 1}: Loss = {losses.item():.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error during validation forward pass on batch {batch_idx + 1}: {e}\")\n",
    "                continue\n",
    "            finally:\n",
    "                model.eval()  # Switch back to evaluation mode\n",
    "    \n",
    "    # Compute average validation loss\n",
    "    avg_val_loss = val_loss / val_batches if val_batches > 0 else float('inf')\n",
    "    validation_losses.append(avg_val_loss)\n",
    "    print(f\"Epoch {epoch + 1} Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Adjust learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save the model after each epoch\n",
    "    model_filename = f\"model_epoch_{epoch + 1}.pth\"\n",
    "    model_save_path = os.path.join(models_dir, model_filename)\n",
    "    if model_save == True or epoch = (num_epochs - 1):\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_train_loss,\n",
    "        }, model_save_path)\n",
    "        print(f\"Saved model at {model_save_path}\")\n",
    "    \n",
    "# After training, plot the training and validation losses\n",
    "plt.plot(range(1, num_epochs + 1), training_losses, label='Training Loss')\n",
    "plt.plot(range(1, num_epochs + 1), validation_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096ff655-abf6-4546-9545-f39da8420e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, plot the training and validation losses\n",
    "plt.plot(range(1, num_epochs + 1), training_losses, label='Training Loss')\n",
    "plt.plot(range(1, num_epochs + 1), validation_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c349930b-3ee6-4f0e-a700-2cf34d3a17c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation losses are stored to check if the model is being trained correctly and is not overfitting \n",
    "with open(r\"C:\\Users\\leona\\0_etch-pits-segmentation_Mask-CNN\\etch pits test\\Models\\training_losses.txt\", \"w\") as output:\n",
    "   for row in training_losses:\n",
    "        output.write(str(row) + '\\n')\n",
    "with open(r\"C:\\Users\\leona\\0_etch-pits-segmentation_Mask-CNN\\etch pits test\\Models\\validation_losses.txt\", \"w\") as output:\n",
    "   for row in validation_losses:\n",
    "        output.write(str(row) + '\\n')\n",
    "loss_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5778c4-e8ec-49ac-b42e-9bdd02b450f8",
   "metadata": {},
   "source": [
    "#### Part 5(optional): Re-load model\n",
    "- Once the model's weights are calculated from the training and stored, they can be quickly reloaded without going through training again.\n",
    "- If the notebook is is to be used only to predict (without training), also the pre-trained model needs to be re-loaded (see Part 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12518b5d-491d-4fc8-a295-38e137529263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load weights and check that all model's keys are matched\n",
    "weights_path = r\"C:\\Users\\leona\\0_etch-pits-segmentation_Mask-CNN\\etch pits test\\Models\\model4_512x512_3x5 grid\\model_epoch_30.pth\"\n",
    "state_dict = torch.load(weights_path, map_location=device)\n",
    "model.load_state_dict(state_dict['model_state_dict'], strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a03354-852c-4a13-a187-bff438b0054d",
   "metadata": {},
   "source": [
    "# Part 6: Visualize predictions\n",
    "- Visualize predictions using cropped images in the test dataloader.\n",
    "- The same functions can now be used to analyze any data, including fresh new SEM images from the lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752a80da-3602-497f-8c9e-b9d533442a98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here we use the test_dataloader (for which we have a manually labelled ground truth)\n",
    "# to visually and quantitatively assess the validity of the model's predictions\n",
    "\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "ious = []  # To store IoU values\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, targets in test_loader:\n",
    "        # Skip empty batches\n",
    "        if len(images) == 0 or len(targets) == 0:\n",
    "            print(\"Skipping empty batch during testing...\")\n",
    "            continue\n",
    "        \n",
    "        # Move images to device\n",
    "        images = [img.to(device) for img in images]\n",
    "\n",
    "        # Model predictions\n",
    "        outputs = model(images)\n",
    "        outputs = apply_nms(outputs, iou_threshold=1)\n",
    "\n",
    "        for idx, output in enumerate(outputs):\n",
    "            # Original image\n",
    "            original_image = F.to_pil_image(images[idx].cpu())\n",
    "\n",
    "            # Predicted masks: Combine all into one binary mask\n",
    "            pred_masks = output['masks'].squeeze(1) > 0.7  # Shape: [N_pred, H, W]\n",
    "            combined_pred_mask = torch.any(pred_masks, dim=0).cpu().numpy()  # Combine masks: [H, W]\n",
    "            \n",
    "            # Ground truth masks\n",
    "            true_masks = targets[idx]['masks'].cpu().numpy().astype(int)  # Shape: [N_true, H, W]\n",
    "            combined_true_mask = np.any(true_masks, axis=0).astype(int)  # Combine masks: [H, W]\n",
    "\n",
    "            # IoU calculation\n",
    "            iou = jaccard_score(combined_true_mask.flatten(), combined_pred_mask.flatten())\n",
    "            ious.append(iou)\n",
    "        \n",
    "            # Bounding boxes\n",
    "            pred_boxes = output['boxes'].cpu().numpy()  # Shape: [N_pred, 4]\n",
    "            pred_scores = output['scores'].cpu().numpy()  # Shape: [N_pred]\n",
    "            true_boxes = targets[idx]['boxes'].cpu().numpy()\n",
    "\n",
    "            # Convert bounding boxes to binary masks for comparison\n",
    "            pred_box_mask = np.zeros_like(combined_true_mask, dtype=int)\n",
    "            true_box_mask = np.zeros_like(combined_true_mask, dtype=int)\n",
    "\n",
    "            for box in pred_boxes:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                pred_box_mask[y1:y2, x1:x2] = 1\n",
    "\n",
    "            for box in true_boxes:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                true_box_mask[y1:y2, x1:x2] = 1\n",
    "            \n",
    "            precision = precision_score(true_box_mask.flatten(), pred_box_mask.flatten())\n",
    "            recall = recall_score(true_box_mask.flatten(), pred_box_mask.flatten())\n",
    "            f1 = f1_score(true_box_mask.flatten(), pred_box_mask.flatten())\n",
    "\n",
    "            precision_list.append(precision)\n",
    "            recall_list.append(recall)\n",
    "            f1_list.append(f1)  \n",
    "            \n",
    "            # Create a plot\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "            # Plot original image\n",
    "            ax[0].imshow(original_image, cmap=\"gray\")\n",
    "            ax[0].set_title(\"Original Image\")\n",
    "            ax[0].axis(\"off\")\n",
    "\n",
    "            # Plot combined mask and bounding boxes\n",
    "            ax[1].imshow(original_image, alpha=0.8)\n",
    "            ax[1].imshow(combined_pred_mask, cmap='grey', vmin=0, vmax=1, alpha=0.5)\n",
    "            for box, score in zip(pred_boxes, pred_scores):\n",
    "                if score > 0.5:  # Confidence threshold\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                         edgecolor='red', facecolor='none', linewidth=2)\n",
    "                    ax[1].add_patch(rect)\n",
    "            ax[1].set_title(\"Predicted Masks and Bounding Boxes\")\n",
    "            ax[1].axis(\"off\")\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "# Print average metrics\n",
    "print(f\"Average IoU: {np.mean(ious):.4f}\")\n",
    "print(f\"Average Precision: {np.mean(precision_list):.4f}\")\n",
    "print(f\"Average Recall: {np.mean(recall_list):.4f}\")\n",
    "print(f\"Average F1-Score: {np.mean(f1_list):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0655e007-1b02-489c-9f12-bec2de86beab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print average metrics\n",
    "print(f\"Average IoU: {np.mean(ious):.4f}\")\n",
    "print(f\"Average Precision: {np.mean(precision_list):.4f}\")\n",
    "print(f\"Average Recall: {np.mean(recall_list):.4f}\")\n",
    "print(f\"Average F1-Score: {np.mean(f1_list):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de705e67-8bca-4bad-a381-7009d3a40516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the predicted masks are visualized independently and with a score. Can be used for debugging if predictions are very far off.\n",
    "\n",
    "for idx, output in enumerate(outputs):\n",
    "    print(f\"Image {idx}: Number of predicted boxes = {len(output['boxes'])}\")\n",
    "    print(f\"Image {idx}: Number of predicted masks = {len(output['masks'])}\")\n",
    "\n",
    "    if len(output[\"boxes\"]) == 0:\n",
    "        print(\"No boxes found for this image. Skipping visualization.\")\n",
    "        continue\n",
    "\n",
    "    # Plot the image, masks, and bounding boxes\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "    ax.set_title(f\"Image {idx}: Predicted Masks and Bounding Boxes\")\n",
    "    ax.axis(\"off\")\n",
    "    \n",
    "    # Visualize masks\n",
    "    pred_masks = output[\"masks\"].squeeze(1).cpu().numpy()\n",
    "    boxes = output['boxes'].cpu().numpy()  # Shape: [N_pred, 4]\n",
    "    scores = output['scores'].cpu().numpy()  # Shape: [N_pred]\n",
    "    \n",
    "    # Visualize each mask and bounding box\n",
    "    for i, (mask, box, score) in enumerate(zip(pred_masks, boxes, scores)):\n",
    "        if score > 0.5:  # Confidence threshold\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "            ax.set_title(f\"Image {idx} - Mask {i} (Score: {score:.2f})\")\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "            # Overlay the mask\n",
    "            ax.imshow(mask, alpha=0.7, cmap=\"jet\")  # Adjust alpha for transparency\n",
    "\n",
    "            # Draw the bounding box\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                  edgecolor='red', facecolor='none', linewidth=2)\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Annotate with the score\n",
    "            ax.text(x1, y1 - 10, f\"{score:.2f}\", color='red', fontsize=10,\n",
    "                    bbox=dict(facecolor='white', alpha=0.5, edgecolor='none'))\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a784b9-5e7a-43ee-a599-f769e08610a3",
   "metadata": {},
   "source": [
    "### Full-image predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffbc3b3-2f83-4668-a02d-0e47fe108d59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Paper images (partly seen by training)\n",
    "test_dir = r\"C:\\Users\\leona\\0_etch-pits-segmentation_Mask-CNN\\etch pits test\\Training\\analysis\\images_tiff_batch-size8\"\n",
    "test_dir_pred = r\"C:\\Users\\leona\\0_etch-pits-segmentation_Mask-CNN\\etch pits test\\Training\\analysis\\images_tiff_batch-size8\\predictions\"\n",
    "test_images_pred = r\"C:\\Users\\leona\\0_etch-pits-segmentation_Mask-CNN\\etch pits test\\Training\\analysis\\images_tiff_batch-size8\\images_predictions\"\n",
    "\n",
    "test_paths = sorted(glob.glob(os.path.join(test_dir, \"image_*\")))\n",
    "for test_image_path in test_paths:\n",
    "\n",
    "    # Load the test image\n",
    "    sum_final_boxes = []\n",
    "    sum_final_center_x = []\n",
    "    sum_final_center_y = []\n",
    "    sum_final_scores = []\n",
    "    sum_final_masks = []\n",
    "    sum_sides = []\n",
    "    measurements_unit = \"Âµm\"\n",
    "    test_image_idx = os.path.basename(test_image_path).split(\"_\")[1].split(\".\")[0]  # Extract 'i' from 'image_i'\n",
    "    test_image = Image.open(test_image_path).convert(\"I\")\n",
    "    #x_res = test_image.pages[0].tags['XResolution'].value\n",
    "    x_res = test_image.info[\"resolution\"][0]\n",
    "    pixel_size_x = 1 / x_res\n",
    "    \n",
    "    # Convert to tensor and normalize\n",
    "    test_image_tensor = transforms.ToTensor()(test_image)\n",
    "    test_image_tensor = normalize_image(test_image_tensor)\n",
    "    \n",
    "    # Add batch dimension\n",
    "    test_image_tensor = test_image_tensor.unsqueeze(0).to(device)\n",
    "    \n",
    "    test_crops = split_image_and_targets(test_image_tensor[0],[],[],[], grid_size=(3, 5), crop_size=(512, 512))\n",
    "    \n",
    "    # Initialize blank canvas for reconstruction\n",
    "    H, W = test_image_tensor.shape[2:]\n",
    "    reconstructed_mask = np.zeros((H, W), dtype=np.float32)\n",
    "    all_boxes = []\n",
    "    all_scores = []\n",
    "    all_masks = []\n",
    "    \n",
    "    # Forward pass\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, crop in enumerate(test_crops):\n",
    "            # Prepare the crop\n",
    "            image = crop['image']\n",
    "            image = image.unsqueeze(0).to(device)  # Add batch and channel dimensions\n",
    "            y_start, x_start = crop['metadata']['y_start'], crop['metadata']['x_start']\n",
    "    \n",
    "            # Model predictions\n",
    "            test_outputs = model(image)\n",
    "            #test_outputs = apply_nms(test_outputs, iou_threshold=0.)\n",
    "    \n",
    "            # Extract predictions\n",
    "            output = test_outputs[0]\n",
    "            masks = output['masks'].squeeze(1).cpu().numpy() if 'masks' in output else []\n",
    "            boxes = output['boxes'].cpu().numpy() if 'boxes' in output else []\n",
    "            scores = output['scores'].cpu().numpy() if 'scores' in output else []\n",
    "    \n",
    "            # Store predictions for merging\n",
    "            for mask, box, score in zip(masks, boxes, scores):\n",
    "                if score > 0.5:  # Confidence threshold\n",
    "                    # Adjust bounding boxes to original coordinates\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    adjusted_box = [x1 + x_start, y1 + y_start, x2 + x_start, y2 + y_start]\n",
    "                    all_boxes.append(adjusted_box)\n",
    "                    all_scores.append(score)\n",
    "    \n",
    "                    # Place mask back in original coordinates\n",
    "                    adjusted_mask = np.zeros((H, W), dtype=np.float32)\n",
    "                    y_slice = slice(y_start, y_start + mask.shape[0])\n",
    "                    x_slice = slice(x_start, x_start + mask.shape[1])\n",
    "                    adjusted_mask[y_slice, x_slice] = mask\n",
    "                    all_masks.append(adjusted_mask)\n",
    "    \n",
    "    # Convert lists to tensors for NMS\n",
    "    all_boxes = torch.tensor(all_boxes, dtype=torch.float32)\n",
    "    all_scores = torch.tensor(all_scores, dtype=torch.float32)\n",
    "    if len(all_boxes) > 0:\n",
    "        # Apply NMS to filter overlapping boxes\n",
    "        keep_indices = nms(all_boxes, all_scores, iou_threshold=0.3)\n",
    "    \n",
    "        # Filter boxes, scores, and masks based on NMS results\n",
    "        final_boxes = all_boxes[keep_indices].numpy()\n",
    "        final_scores = all_scores[keep_indices].numpy()\n",
    "        final_masks = [all_masks[i] for i in keep_indices]\n",
    "    \n",
    "        # Combine masks into the reconstructed image\n",
    "        for mask in final_masks:\n",
    "            reconstructed_mask = np.maximum(reconstructed_mask, mask)\n",
    "    \n",
    "        # Plot the original image with overlaid predictions\n",
    "        plt.imsave((os.path.join(test_dir_pred,f\"image_{test_image_idx}.png\")), reconstructed_mask, cmap='gray')\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(test_image_tensor.cpu().squeeze(), cmap='gray', alpha=0.5)\n",
    "        plt.imshow(reconstructed_mask, cmap='gray', alpha=0.5)\n",
    "        \n",
    "        for box in final_boxes:\n",
    "            x1, y1, x2, y2 = box\n",
    "            center_x = x1 + (x2 - x1)/2\n",
    "            center_y = y1 + (y2 - y1)/2 \n",
    "            rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, edgecolor='red', facecolor='none', linewidth=2)\n",
    "            plt.gca().add_patch(rect)\n",
    "            area = 0.5 * (x2 - x1) * (y2 - y1) * (pixel_size_x**2)\n",
    "            size = np.sqrt((4/np.sqrt(3))*area)\n",
    "            sum_final_boxes.append(box)\n",
    "            sum_final_center_x.append(center_x)\n",
    "            sum_final_center_y.append(center_y)\n",
    "            sum_sides.append(size)\n",
    "\n",
    "        # Save the pits data as CSV file and the final image as a PNG file\n",
    "        sum_dict = {'x':sum_final_center_x, 'y':sum_final_center_y, 'side': sum_sides}\n",
    "        df = pd.DataFrame(sum_dict) \n",
    "        save_path_data = os.path.join(test_images_pred, f\"image_{test_image_idx}.csv\")\n",
    "        df.to_csv(save_path_data)\n",
    "        plt.axis('off')  # Turn off the axis for better visualization\n",
    "        save_path_img = os.path.join(test_images_pred, f\"image_{test_image_idx}.png\")\n",
    "        plt.savefig(save_path_img, bbox_inches='tight', pad_inches=0)                                    \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(sum_sides, bins=np.arange(0, 0.45 + 0.02, 0.02), color='blue', alpha=0.7, edgecolor='black', weights=np.ones_like(sum_sides) / len(sum_sides))\n",
    "        plt.title(\"Histogram of image \"+str(test_image_idx), fontsize=16)\n",
    "        plt.xlabel(\"Size (Âµm)\", fontsize=14)\n",
    "        plt.ylabel(\"Relative frequency\", fontsize=14)\n",
    "        ax = plt.gca()\n",
    "        ax.set_xlim([0, 0.45])\n",
    "        ax.set_ylim([0, 0.45])\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d80fb6-bf97-4cbe-90ba-836275fd7929",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Images not seen during training\n",
    "\n",
    "test_dir = r\"C:\\Users\\leona\\0_etch-pits-segmentation_Mask-CNN\\etch pits test\\Training\\analysis\\images_tiff_batch-size8\\Unseen_images\"\n",
    "test_dir_pred = r\"C:\\Users\\leona\\0_etch-pits-segmentation_Mask-CNN\\etch pits test\\Training\\analysis\\images_tiff_batch-size8\\Unseen_images\\Unseen_predictions\"\n",
    "test_images_pred = r\"C:\\Users\\leona\\0_etch-pits-segmentation_Mask-CNN\\etch pits test\\Training\\analysis\\images_tiff_batch-size8\\Unseen_images\\Unseen_images_predictions\"\n",
    "\n",
    "test_paths = sorted(glob.glob(os.path.join(test_dir, \"image_*\")))\n",
    "for test_image_path in test_paths:\n",
    "\n",
    "    # Load the test image\n",
    "    sum_final_boxes = []\n",
    "    sum_final_center_x = []\n",
    "    sum_final_center_y = []\n",
    "    sum_final_scores = []\n",
    "    sum_final_masks = []\n",
    "    sum_sides = []\n",
    "    measurements_unit = \"Âµm\"\n",
    "    test_image_idx = os.path.basename(test_image_path).split(\"_\")[1].split(\".\")[0]  # Extract 'i' from 'image_i'\n",
    "    test_image = Image.open(test_image_path).convert(\"I\")\n",
    "    #x_res = test_image.pages[0].tags['XResolution'].value\n",
    "    x_res = test_image.info[\"resolution\"][0]\n",
    "    pixel_size_x = 1 / x_res\n",
    "    \n",
    "    # Convert to tensor and normalize\n",
    "    test_image_tensor = transforms.ToTensor()(test_image)\n",
    "    test_image_tensor = normalize_image(test_image_tensor)\n",
    "    \n",
    "    # Add batch dimension\n",
    "    test_image_tensor = test_image_tensor.unsqueeze(0).to(device)\n",
    "    \n",
    "    test_crops = split_image_and_targets(test_image_tensor[0],[],[],[], grid_size=(3, 5), crop_size=(512, 512))\n",
    "    \n",
    "    # Initialize blank canvas for reconstruction\n",
    "    H, W = test_image_tensor.shape[2:]\n",
    "    reconstructed_mask = np.zeros((H, W), dtype=np.float32)\n",
    "    all_boxes = []\n",
    "    all_scores = []\n",
    "    all_masks = []\n",
    "    \n",
    "    # Forward pass\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, crop in enumerate(test_crops):\n",
    "            # Prepare the crop\n",
    "            image = crop['image']\n",
    "            image = image.unsqueeze(0).to(device)  # Add batch and channel dimensions\n",
    "            y_start, x_start = crop['metadata']['y_start'], crop['metadata']['x_start']\n",
    "    \n",
    "            # Model predictions\n",
    "            test_outputs = model(image)\n",
    "            #test_outputs = apply_nms(test_outputs, iou_threshold=0.)\n",
    "    \n",
    "            # Extract predictions\n",
    "            output = test_outputs[0]\n",
    "            masks = output['masks'].squeeze(1).cpu().numpy() if 'masks' in output else []\n",
    "            boxes = output['boxes'].cpu().numpy() if 'boxes' in output else []\n",
    "            scores = output['scores'].cpu().numpy() if 'scores' in output else []\n",
    "    \n",
    "            # Store predictions for merging\n",
    "            for mask, box, score in zip(masks, boxes, scores):\n",
    "                if score > 0.5:  # Confidence threshold\n",
    "                    # Adjust bounding boxes to original coordinates\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    adjusted_box = [x1 + x_start, y1 + y_start, x2 + x_start, y2 + y_start]\n",
    "                    all_boxes.append(adjusted_box)\n",
    "                    all_scores.append(score)\n",
    "    \n",
    "                    # Place mask back in original coordinates\n",
    "                    adjusted_mask = np.zeros((H, W), dtype=np.float32)\n",
    "                    y_slice = slice(y_start, y_start + mask.shape[0])\n",
    "                    x_slice = slice(x_start, x_start + mask.shape[1])\n",
    "                    adjusted_mask[y_slice, x_slice] = mask\n",
    "                    all_masks.append(adjusted_mask)\n",
    "    \n",
    "    # Convert lists to tensors for NMS\n",
    "    all_boxes = torch.tensor(all_boxes, dtype=torch.float32)\n",
    "    all_scores = torch.tensor(all_scores, dtype=torch.float32)\n",
    "    if len(all_boxes) > 0:\n",
    "        # Apply NMS to filter overlapping boxes\n",
    "        keep_indices = nms(all_boxes, all_scores, iou_threshold=0.3)\n",
    "    \n",
    "        # Filter boxes, scores, and masks based on NMS results\n",
    "        final_boxes = all_boxes[keep_indices].numpy()\n",
    "        final_scores = all_scores[keep_indices].numpy()\n",
    "        final_masks = [all_masks[i] for i in keep_indices]\n",
    "    \n",
    "        # Combine masks into the reconstructed image\n",
    "        for mask in final_masks:\n",
    "            reconstructed_mask = np.maximum(reconstructed_mask, mask)\n",
    "    \n",
    "        # Plot the original image with overlaid predictions\n",
    "        plt.imsave((os.path.join(test_dir_pred,f\"image_{test_image_idx}.png\")), reconstructed_mask, cmap='gray')\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(test_image_tensor.cpu().squeeze(), cmap='gray', alpha=0.5)\n",
    "        plt.imshow(reconstructed_mask, cmap='gray', alpha=0.5)\n",
    "        \n",
    "        for box in final_boxes:\n",
    "            x1, y1, x2, y2 = box\n",
    "            center_x = x1 + (x2 - x1)/2\n",
    "            center_y = y1 + (y2 - y1)/2 \n",
    "            rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, edgecolor='red', facecolor='none', linewidth=2)\n",
    "            plt.gca().add_patch(rect)\n",
    "            area = 0.5 * (x2 - x1) * (y2 - y1) * (pixel_size_x**2)\n",
    "            size = np.sqrt((4/np.sqrt(3))*area)\n",
    "            sum_final_boxes.append(box)\n",
    "            sum_final_center_x.append(center_x)\n",
    "            sum_final_center_y.append(center_y)\n",
    "            sum_sides.append(size)\n",
    "\n",
    "        # Save the pits data as CSV file and the final image as a PNG file\n",
    "        sum_dict = {'x':sum_final_center_x, 'y':sum_final_center_y, 'side': sum_sides}\n",
    "        df = pd.DataFrame(sum_dict) \n",
    "        save_path_data = os.path.join(test_images_pred, f\"image_{test_image_idx}.csv\")\n",
    "        df.to_csv(save_path_data)\n",
    "        plt.axis('off')  # Turn off the axis for better visualization\n",
    "        save_path_img = os.path.join(test_images_pred, f\"image_{test_image_idx}.png\")\n",
    "        plt.savefig(save_path_img, bbox_inches='tight', pad_inches=0)                                    \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(sum_sides, bins=np.arange(0, 0.45 + 0.02, 0.02), color='blue', alpha=0.7, edgecolor='black', weights=np.ones_like(sum_sides) / len(sum_sides))\n",
    "        plt.title(\"Histogram of image \"+str(test_image_idx), fontsize=16)\n",
    "        plt.xlabel(\"Size (Âµm)\", fontsize=14)\n",
    "        plt.ylabel(\"Relative frequency\", fontsize=14)\n",
    "        ax = plt.gca()\n",
    "        ax.set_xlim([0, 0.45])\n",
    "        ax.set_ylim([0, 0.45])\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9da856-dffb-48de-918b-f437cc258762",
   "metadata": {},
   "source": [
    "### End of notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
